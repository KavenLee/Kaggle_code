{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "from keras.layers.core import Dense, Activation,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras. callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk. corpus import stopwords\n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('C:data/spooky/train.csv')\n",
    "test=pd.read_csv('C:data/spooky/test.csv')\n",
    "sample=pd.read_csv('C:data/spooky/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Multi class version of Logarithmic Loss metrinc.\n",
    "    :param actual : Array containing the actual target classes\n",
    "    :param predicted : Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert 'actual' to a binary array if it;s not already:\n",
    "    if len(actual.shape)==1:\n",
    "        actual2=np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[1,val]=1\n",
    "        actual=actual2\n",
    "        \n",
    "    clip=np.clip(predicted,eps,1-eps)\n",
    "    rows=actual.shape[0]\n",
    "    vsota = np.sum(actual*np.log(clip))\n",
    "    return -1.0/rows*vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xvalid,ytrain,yvalid = train_test_split(train.text.values, y, \n",
    "                                              stratify=y,\n",
    "                                              random_state=42,\n",
    "                                              test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Basic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss : 0.002 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv,ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print('logloss : %0.3f '% multiclass_logloss(yvalid,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',ngram_range=(1,3),\n",
    "                     stop_words='english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain)+list(xvalid))\n",
    "xtrain_ctv = ctv.transform(xtrain)\n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss : 0.003 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv,ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print('logloss : %0.3f ' % multiclass_logloss(yvalid,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.002 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.003 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on Counts\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.002 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple SVM\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:02:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.002 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:02:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.002 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_ctv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:04:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.003 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:05:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.002 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(multiclass_logloss,greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SVD\n",
    "svd = TruncatedSVD()\n",
    "\n",
    "# Initialize the standard scaler\n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "# We will use logistic regression here..\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Create the pipeline\n",
    "clf = pipeline.Pipeline([('svd',svd),\n",
    "                        ('scl',scl),\n",
    "                        ('lr',lr_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120,180],\n",
    "             'lr__C':[0.1,1.0,10],\n",
    "             'lr__penalty':['l1','l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [        nan         nan -0.00045471 -0.00049939         nan         nan\n",
      " -0.00043893 -0.00050961         nan         nan -0.00047613 -0.00044943]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score : -0.000\n",
      "Best parameters set : \n",
      "\tlr__C : 1.0\n",
      "\tlr__penalty : 'l2'\n",
      "\tsvd__n_components : 120\n"
     ]
    }
   ],
   "source": [
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf,param_grid=param_grid,scoring=mll_scorer,\n",
    "                    verbose=10, n_jobs=-1,  refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv,ytrain) # we can use the full data here but im only using xtrain\n",
    "print('Best score : %0.3f' % model.best_score_)\n",
    "print('Best parameters set : ')\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print('\\t%s : %r' %(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "[CV 1/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 1/2; 1/6] END .............nb__alpha=0.001;, score=-0.001 total time=   0.0s\n",
      "[CV 2/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 2/2; 1/6] END .............nb__alpha=0.001;, score=-0.001 total time=   0.0s\n",
      "[CV 1/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 1/2; 2/6] END ..............nb__alpha=0.01;, score=-0.001 total time=   0.0s\n",
      "[CV 2/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 2/2; 2/6] END ..............nb__alpha=0.01;, score=-0.001 total time=   0.0s\n",
      "[CV 1/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 1/2; 3/6] END ...............nb__alpha=0.1;, score=-0.001 total time=   0.0s\n",
      "[CV 2/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 2/2; 3/6] END ...............nb__alpha=0.1;, score=-0.001 total time=   0.0s\n",
      "[CV 1/2; 4/6] START nb__alpha=1.................................................\n",
      "[CV 1/2; 4/6] END .................nb__alpha=1;, score=-0.000 total time=   0.0s\n",
      "[CV 2/2; 4/6] START nb__alpha=1.................................................\n",
      "[CV 2/2; 4/6] END .................nb__alpha=1;, score=-0.000 total time=   0.0s\n",
      "[CV 1/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 1/2; 5/6] END ................nb__alpha=10;, score=-0.000 total time=   0.0s\n",
      "[CV 2/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 2/2; 5/6] END ................nb__alpha=10;, score=-0.000 total time=   0.0s\n",
      "[CV 1/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 1/2; 6/6] END ...............nb__alpha=100;, score=-0.000 total time=   0.0s\n",
      "[CV 2/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 2/2; 6/6] END ...............nb__alpha=100;, score=-0.000 total time=   0.0s\n",
      "Best score: -0.000\n",
      "Best parameters set:\n",
      "\tnb__alpha: 100\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=1, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52343it [00:04, 10988.11it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-a18420edfa6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcoefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0membeddings_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '.'"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('C:data/spooky/glove.840B.300d.txt','rt',encoding='utf8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/17621 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▉                                                                           | 231/17621 [00:00<00:07, 2292.59it/s]\u001b[A\n",
      "  3%|██▌                                                                         | 603/17621 [00:00<00:06, 2586.88it/s]\u001b[A\n",
      "  6%|████▏                                                                       | 979/17621 [00:00<00:05, 2849.27it/s]\u001b[A\n",
      "  8%|█████▋                                                                     | 1342/17621 [00:00<00:05, 3040.44it/s]\u001b[A\n",
      " 10%|███████▏                                                                   | 1696/17621 [00:00<00:05, 3168.52it/s]\u001b[A\n",
      " 12%|████████▊                                                                  | 2085/17621 [00:00<00:04, 3348.99it/s]\u001b[A\n",
      " 14%|██████████▍                                                                | 2446/17621 [00:00<00:04, 3415.85it/s]\u001b[A\n",
      " 16%|███████████▉                                                               | 2806/17621 [00:00<00:04, 3462.07it/s]\u001b[A\n",
      " 18%|█████████████▌                                                             | 3184/17621 [00:00<00:04, 3544.07it/s]\u001b[A\n",
      " 20%|███████████████▏                                                           | 3557/17621 [00:01<00:03, 3590.36it/s]\u001b[A\n",
      " 22%|████████████████▊                                                          | 3941/17621 [00:01<00:03, 3654.03it/s]\u001b[A\n",
      " 25%|██████████████████▍                                                        | 4331/17621 [00:01<00:03, 3716.75it/s]\u001b[A\n",
      " 27%|████████████████████                                                       | 4702/17621 [00:01<00:03, 3673.67it/s]\u001b[A\n",
      " 29%|█████████████████████▋                                                     | 5101/17621 [00:01<00:03, 3755.38it/s]\u001b[A\n",
      " 31%|███████████████████████▎                                                   | 5478/17621 [00:01<00:03, 3751.92it/s]\u001b[A\n",
      " 33%|████████████████████████▉                                                  | 5854/17621 [00:01<00:03, 3666.47it/s]\u001b[A\n",
      " 35%|██████████████████████████▍                                                | 6226/17621 [00:01<00:03, 3674.41it/s]\u001b[A\n",
      " 37%|████████████████████████████                                               | 6598/17621 [00:01<00:02, 3679.97it/s]\u001b[A\n",
      " 40%|█████████████████████████████▋                                             | 6975/17621 [00:01<00:02, 3698.56it/s]\u001b[A\n",
      " 42%|███████████████████████████████▎                                           | 7353/17621 [00:02<00:02, 3714.60it/s]\u001b[A\n",
      " 44%|████████████████████████████████▉                                          | 7730/17621 [00:02<00:02, 3729.34it/s]\u001b[A\n",
      " 46%|██████████████████████████████████▌                                        | 8107/17621 [00:02<00:02, 3733.56it/s]\u001b[A\n",
      " 48%|████████████████████████████████████                                       | 8486/17621 [00:02<00:02, 3742.01it/s]\u001b[A\n",
      " 50%|█████████████████████████████████████▋                                     | 8861/17621 [00:02<00:02, 3736.22it/s]\u001b[A\n",
      " 52%|███████████████████████████████████████▎                                   | 9235/17621 [00:02<00:02, 3696.11it/s]\u001b[A\n",
      " 55%|████████████████████████████████████████▉                                  | 9611/17621 [00:02<00:02, 3712.60it/s]\u001b[A\n",
      " 57%|██████████████████████████████████████████                                | 10003/17621 [00:02<00:02, 3764.24it/s]\u001b[A\n",
      " 59%|███████████████████████████████████████████▌                              | 10380/17621 [00:02<00:01, 3735.45it/s]\u001b[A\n",
      " 61%|█████████████████████████████████████████████▏                            | 10754/17621 [00:02<00:01, 3685.00it/s]\u001b[A\n",
      " 63%|██████████████████████████████████████████████▋                           | 11123/17621 [00:03<00:01, 3634.71it/s]\u001b[A\n",
      " 65%|████████████████████████████████████████████████▎                         | 11509/17621 [00:03<00:01, 3692.10it/s]\u001b[A\n",
      " 67%|█████████████████████████████████████████████████▉                        | 11890/17621 [00:03<00:01, 3725.38it/s]\u001b[A\n",
      " 70%|███████████████████████████████████████████████████▍                      | 12263/17621 [00:03<00:01, 3685.96it/s]\u001b[A\n",
      " 72%|█████████████████████████████████████████████████████                     | 12632/17621 [00:03<00:01, 3614.40it/s]\u001b[A\n",
      " 74%|██████████████████████████████████████████████████████▌                   | 12995/17621 [00:03<00:01, 3547.83it/s]\u001b[A\n",
      " 76%|████████████████████████████████████████████████████████                  | 13351/17621 [00:03<00:01, 3325.80it/s]\u001b[A\n",
      " 78%|█████████████████████████████████████████████████████████▍                | 13690/17621 [00:03<00:01, 3337.60it/s]\u001b[A\n",
      " 80%|███████████████████████████████████████████████████████████               | 14077/17621 [00:03<00:01, 3479.81it/s]\u001b[A\n",
      " 82%|████████████████████████████████████████████████████████████▌             | 14429/17621 [00:03<00:00, 3410.50it/s]\u001b[A\n",
      " 84%|██████████████████████████████████████████████████████████████            | 14773/17621 [00:04<00:00, 3371.83it/s]\u001b[A\n",
      " 86%|███████████████████████████████████████████████████████████████▍          | 15113/17621 [00:04<00:00, 3266.24it/s]\u001b[A\n",
      " 88%|████████████████████████████████████████████████████████████████▊         | 15442/17621 [00:04<00:00, 3015.42it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████████████████████████████████▍       | 15827/17621 [00:04<00:00, 3219.01it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████████▉      | 16187/17621 [00:04<00:00, 3317.82it/s]\u001b[A\n",
      " 94%|█████████████████████████████████████████████████████████████████████▍    | 16527/17621 [00:04<00:00, 3334.91it/s]\u001b[A\n",
      " 96%|██████████████████████████████████████████████████████████████████████▊   | 16874/17621 [00:04<00:00, 3367.42it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████████████████████████████████████▎ | 17214/17621 [00:04<00:00, 2641.14it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 17621/17621 [00:05<00:00, 3475.74it/s]\u001b[A\n",
      "\n",
      "  0%|                                                                                         | 0/1958 [00:00<?, ?it/s]\u001b[A\n",
      " 18%|██████████████                                                               | 356/1958 [00:00<00:00, 3533.07it/s]\u001b[A\n",
      " 27%|████████████████████▉                                                        | 532/1958 [00:00<00:00, 2704.37it/s]\u001b[A\n",
      " 41%|███████████████████████████████▎                                             | 796/1958 [00:00<00:00, 2678.77it/s]\u001b[A\n",
      " 59%|█████████████████████████████████████████████▏                              | 1163/1958 [00:00<00:00, 2909.89it/s]\u001b[A\n",
      " 77%|██████████████████████████████████████████████████████████▋                 | 1511/1958 [00:00<00:00, 3054.30it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1958/1958 [00:00<00:00, 3081.96it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:54:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:54:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.003 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:57:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:57:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.002 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learnning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "276/276 [==============================] - 19s 7ms/step - loss: 1.0775 - val_loss: 0.7872\n",
      "Epoch 2/5\n",
      "276/276 [==============================] - 2s 6ms/step - loss: 0.7453 - val_loss: 0.7484\n",
      "Epoch 3/5\n",
      "276/276 [==============================] - 2s 5ms/step - loss: 0.6817 - val_loss: 0.7416\n",
      "Epoch 4/5\n",
      "276/276 [==============================] - 2s 6ms/step - loss: 0.6473 - val_loss: 0.7282\n",
      "Epoch 5/5\n",
      "276/276 [==============================] - 1s 5ms/step - loss: 0.6058 - val_loss: 0.7093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1daf35f9dc0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████| 25943/25943 [00:00<00:00, 406451.77it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 39s 798ms/step - loss: 1.0918 - val_loss: 0.9453\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 28s 788ms/step - loss: 0.9561 - val_loss: 0.8041\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 28s 802ms/step - loss: 0.8685 - val_loss: 0.7527\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 27s 778ms/step - loss: 0.8165 - val_loss: 0.7300\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 28s 788ms/step - loss: 0.8024 - val_loss: 0.7240\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 28s 797ms/step - loss: 0.7821 - val_loss: 0.6996\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 27s 780ms/step - loss: 0.7615 - val_loss: 0.6885\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 28s 798ms/step - loss: 0.7417 - val_loss: 0.6971\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 27s 781ms/step - loss: 0.7338 - val_loss: 0.6617\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 28s 793ms/step - loss: 0.7080 - val_loss: 0.6501\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 28s 799ms/step - loss: 0.6874 - val_loss: 0.6424\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 27s 784ms/step - loss: 0.6879 - val_loss: 0.6520\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 28s 803ms/step - loss: 0.6723 - val_loss: 0.7015\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 28s 800ms/step - loss: 0.6684 - val_loss: 0.6158\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 27s 777ms/step - loss: 0.6248 - val_loss: 0.6122\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 27s 784ms/step - loss: 0.6272 - val_loss: 0.6025\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 28s 797ms/step - loss: 0.6070 - val_loss: 0.6029\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 28s 798ms/step - loss: 0.5903 - val_loss: 0.5922\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 28s 799ms/step - loss: 0.5823 - val_loss: 0.6070\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 27s 782ms/step - loss: 0.5691 - val_loss: 0.6070\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 28s 786ms/step - loss: 0.5492 - val_loss: 0.6005\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 27s 765ms/step - loss: 0.5525 - val_loss: 0.5778\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 28s 799ms/step - loss: 0.5349 - val_loss: 0.5739\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 28s 789ms/step - loss: 0.5227 - val_loss: 0.6000\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 28s 799ms/step - loss: 0.5159 - val_loss: 0.5804\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 28s 792ms/step - loss: 0.5111 - val_loss: 0.5847\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 27s 783ms/step - loss: 0.4907 - val_loss: 0.5930\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 28s 789ms/step - loss: 0.4811 - val_loss: 0.5933\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 28s 796ms/step - loss: 0.4689 - val_loss: 0.5818\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 28s 805ms/step - loss: 0.4756 - val_loss: 0.6082\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 28s 788ms/step - loss: 0.4806 - val_loss: 0.5908\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 28s 800ms/step - loss: 0.4431 - val_loss: 0.5938\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 29s 819ms/step - loss: 0.4510 - val_loss: 0.5834\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 30s 853ms/step - loss: 0.4512 - val_loss: 0.5886\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 28s 810ms/step - loss: 0.4308 - val_loss: 0.5860\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 28s 789ms/step - loss: 0.4228 - val_loss: 0.5893\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 29s 826ms/step - loss: 0.4263 - val_loss: 0.5772\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 28s 801ms/step - loss: 0.4208 - val_loss: 0.5850\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 28s 801ms/step - loss: 0.4231 - val_loss: 0.5859\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 37s 1s/step - loss: 0.3984 - val_loss: 0.5955\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 30s 853ms/step - loss: 0.4002 - val_loss: 0.6097\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 30s 853ms/step - loss: 0.3993 - val_loss: 0.5916\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 32s 907ms/step - loss: 0.3922 - val_loss: 0.5881\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 29s 817ms/step - loss: 0.3808 - val_loss: 0.5821\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 36s 1s/step - loss: 0.3642 - val_loss: 0.6101\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 28s 809ms/step - loss: 0.3832 - val_loss: 0.6096\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 27s 775ms/step - loss: 0.3608 - val_loss: 0.5912\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 28s 797ms/step - loss: 0.3652 - val_loss: 0.6037\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 27s 777ms/step - loss: 0.3694 - val_loss: 0.6103\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 27s 786ms/step - loss: 0.3533 - val_loss: 0.6216\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 28s 810ms/step - loss: 0.3497 - val_loss: 0.6162\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 29s 820ms/step - loss: 0.3621 - val_loss: 0.6140\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 30s 845ms/step - loss: 0.3369 - val_loss: 0.6114\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 30s 856ms/step - loss: 0.3453 - val_loss: 0.6472\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 29s 813ms/step - loss: 0.3362 - val_loss: 0.6281\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 29s 824ms/step - loss: 0.3280 - val_loss: 0.6227\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 28s 813ms/step - loss: 0.3380 - val_loss: 0.6498\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 30s 860ms/step - loss: 0.3229 - val_loss: 0.6369\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 27s 778ms/step - loss: 0.3264 - val_loss: 0.6122\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 28s 797ms/step - loss: 0.3252 - val_loss: 0.6364\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 27s 761ms/step - loss: 0.3034 - val_loss: 0.6345\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 27s 767ms/step - loss: 0.3147 - val_loss: 0.6631\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 27s 781ms/step - loss: 0.3095 - val_loss: 0.6517\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 27s 772ms/step - loss: 0.3043 - val_loss: 0.6544\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 27s 765ms/step - loss: 0.3158 - val_loss: 0.6684\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 28s 785ms/step - loss: 0.3048 - val_loss: 0.6617\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 28s 789ms/step - loss: 0.3044 - val_loss: 0.6653\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 27s 782ms/step - loss: 0.2891 - val_loss: 0.6524\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 28s 785ms/step - loss: 0.2932 - val_loss: 0.6768\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 27s 777ms/step - loss: 0.2847 - val_loss: 0.6527\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 28s 807ms/step - loss: 0.2790 - val_loss: 0.6665\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 29s 829ms/step - loss: 0.2829 - val_loss: 0.6702\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 27s 779ms/step - loss: 0.2755 - val_loss: 0.6682\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 28s 799ms/step - loss: 0.2796 - val_loss: 0.6599\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 27s 776ms/step - loss: 0.2771 - val_loss: 0.6686\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 27s 777ms/step - loss: 0.2742 - val_loss: 0.6703\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 30s 860ms/step - loss: 0.2819 - val_loss: 0.6505\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 29s 825ms/step - loss: 0.2644 - val_loss: 0.6697\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 29s 818ms/step - loss: 0.2683 - val_loss: 0.6839\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 28s 801ms/step - loss: 0.2561 - val_loss: 0.6743\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 28s 800ms/step - loss: 0.2560 - val_loss: 0.6918\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 28s 800ms/step - loss: 0.2613 - val_loss: 0.6777\n",
      "Epoch 83/100\n",
      "35/35 [==============================] - 28s 803ms/step - loss: 0.2674 - val_loss: 0.7013\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 27s 780ms/step - loss: 0.2628 - val_loss: 0.6936\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 28s 801ms/step - loss: 0.2550 - val_loss: 0.6904\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 28s 812ms/step - loss: 0.2419 - val_loss: 0.7127\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 27s 787ms/step - loss: 0.2423 - val_loss: 0.6949\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 28s 792ms/step - loss: 0.2444 - val_loss: 0.7104\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 28s 792ms/step - loss: 0.2444 - val_loss: 0.7239\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 28s 791ms/step - loss: 0.2619 - val_loss: 0.7123\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 27s 781ms/step - loss: 0.2525 - val_loss: 0.7067\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 28s 800ms/step - loss: 0.2573 - val_loss: 0.7237\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 28s 812ms/step - loss: 0.2478 - val_loss: 0.7252\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 28s 790ms/step - loss: 0.2358 - val_loss: 0.7378\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 28s 803ms/step - loss: 0.2336 - val_loss: 0.6986\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 27s 779ms/step - loss: 0.2400 - val_loss: 0.7035\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 28s 810ms/step - loss: 0.2482 - val_loss: 0.7325\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 28s 795ms/step - loss: 0.2516 - val_loss: 0.7219\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 27s 782ms/step - loss: 0.2403 - val_loss: 0.7334\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 28s 800ms/step - loss: 0.2298 - val_loss: 0.7301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1daee2637f0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 34s 836ms/step - loss: 1.0882 - val_loss: 0.8494\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 29s 818ms/step - loss: 0.8963 - val_loss: 0.7747\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 28s 811ms/step - loss: 0.8334 - val_loss: 0.7681\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 28s 810ms/step - loss: 0.8187 - val_loss: 0.7746\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 29s 836ms/step - loss: 0.8039 - val_loss: 0.7271\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 29s 821ms/step - loss: 0.7686 - val_loss: 0.7154\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 29s 827ms/step - loss: 0.7471 - val_loss: 0.6804\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 29s 821ms/step - loss: 0.7339 - val_loss: 0.6637\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 29s 836ms/step - loss: 0.7012 - val_loss: 0.6685\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 29s 823ms/step - loss: 0.6915 - val_loss: 0.6561\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 29s 819ms/step - loss: 0.6624 - val_loss: 0.6304\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 29s 824ms/step - loss: 0.6477 - val_loss: 0.6298\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 29s 815ms/step - loss: 0.6231 - val_loss: 0.6118\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 28s 815ms/step - loss: 0.6197 - val_loss: 0.5951\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 28s 793ms/step - loss: 0.5886 - val_loss: 0.6118\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 28s 811ms/step - loss: 0.5681 - val_loss: 0.5987\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 29s 826ms/step - loss: 0.5613 - val_loss: 0.5900\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 28s 810ms/step - loss: 0.5389 - val_loss: 0.5795\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 29s 821ms/step - loss: 0.5229 - val_loss: 0.5667\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 28s 803ms/step - loss: 0.5175 - val_loss: 0.5818\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 28s 814ms/step - loss: 0.5090 - val_loss: 0.5739\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 28s 816ms/step - loss: 0.4656 - val_loss: 0.5595\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 28s 810ms/step - loss: 0.4695 - val_loss: 0.5863\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 28s 797ms/step - loss: 0.4380 - val_loss: 0.5647\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 29s 813ms/step - loss: 0.4442 - val_loss: 0.5879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1daeb104e50>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-476427ecdef5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# A simple bidirectional LSTM with glove embeddings and two dense layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m model.add(Embedding(len(word_index) + 1,\n\u001b[0;32m      4\u001b[0m                      \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                      \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
