{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\n\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\ntrain = pd.read_csv('../input/toxic-comment/train.csv').fillna(' ')\ntest = pd.read_csv('../input/toxic-comment/test.csv').fillna(' ')\n\ntrain_text = train['comment_text']\ntest_text = test['comment_text']\nall_text = pd.concat([train_text, test_text])\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    max_features=10000)\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\ntest_word_features = word_vectorizer.transform(test_text)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(2, 6),\n    max_features=50000)\nchar_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_text)\ntest_char_features = char_vectorizer.transform(test_text)\n\ntrain_features = hstack([train_char_features, train_word_features])\ntest_features = hstack([test_char_features, test_word_features])\n\nscores = []\nsubmission = pd.DataFrame.from_dict({'id': test['id']})\nfor class_name in class_names:\n    train_target = train[class_name]\n    classifier = LogisticRegression(C=0.1, solver='sag')\n\n    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n    scores.append(cv_score)\n    print('CV score for class {} is {}'.format(class_name, cv_score))\n\n    classifier.fit(train_features, train_target)\n    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n\nprint('Total CV score is {}'.format(np.mean(scores)))\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-15T02:16:11.325857Z","iopub.execute_input":"2021-08-15T02:16:11.326198Z","iopub.status.idle":"2021-08-15T02:52:43.106965Z","shell.execute_reply.started":"2021-08-15T02:16:11.326169Z","shell.execute_reply":"2021-08-15T02:52:43.105276Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n  warnings.warn(\"The parameter 'stop_words' will not be used\"\n","output_type":"stream"},{"name":"stdout","text":"CV score for class toxic is 0.9692180496735974\nCV score for class severe_toxic is 0.9875919270095609\nCV score for class obscene is 0.9838683395748838\nCV score for class threat is 0.9833766959148177\nCV score for class insult is 0.977423740842879\nCV score for class identity_hate is 0.9739431545319385\nTotal CV score is 0.9792369845912795\n","output_type":"stream"}]},{"cell_type":"markdown","source":"https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams","metadata":{}}]}