{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import IPython\n",
    "from numpy.fft import rfft, irfft\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms(x):\n",
    "    \"\"\"Mean value of signal `x` squared.\n",
    "    :param x: Dynamic quantity.\n",
    "    :returns: Mean squared of `x`.\n",
    "    \"\"\"\n",
    "    return (np.abs(x)**2.0).mean()\n",
    "\n",
    "def normalize(y, x=None):\n",
    "    \"\"\"normalize power in y to a (standard normal) white noise signal.\n",
    "    Optionally normalize to power in signal `x`.\n",
    "    #The mean power of a Gaussian with :math:`\\\\mu=0` and :math:`\\\\sigma=1` is 1.\n",
    "    \"\"\"\n",
    "    #return y * np.sqrt( (np.abs(x)**2.0).mean() / (np.abs(y)**2.0).mean() )\n",
    "    if x is not None:\n",
    "        x = ms(x)\n",
    "    else:\n",
    "        x = 1.0\n",
    "    return y * np.sqrt( x / ms(y) )\n",
    "    #return y * np.sqrt( 1.0 / (np.abs(y)**2.0).mean() )\n",
    "\n",
    "def white_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    return state.randn(N)\n",
    "\n",
    "def pink_noise(N, state=None):\n",
    "\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N%2\n",
    "    X = state.randn(N//2+1+uneven) + 1j * state.randn(N//2+1+uneven)\n",
    "    S = np.sqrt(np.arange(len(X))+1.) # +1 to avoid divide by zero\n",
    "    y = (irfft(X/S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)\n",
    "\n",
    "def blue_noise(N, state=None):\n",
    "    \"\"\"\n",
    "    Blue noise. \n",
    "    \n",
    "    :param N: Amount of samples.\n",
    "    :param state: State of PRNG.\n",
    "    :type state: :class:`np.random.RandomState`\n",
    "    \n",
    "    Power increases with 6 dB per octave.\n",
    "    Power density increases with 3 dB per octave. \n",
    "    \n",
    "    \"\"\"\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N%2\n",
    "    X = state.randn(N//2+1+uneven) + 1j * state.randn(N//2+1+uneven)\n",
    "    S = np.sqrt(np.arange(len(X)))# Filter\n",
    "    y = (irfft(X*S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)\n",
    "\n",
    "def brown_noise(N, state=None):\n",
    "    \"\"\"\n",
    "    Violet noise.\n",
    "    \n",
    "    :param N: Amount of samples.\n",
    "    :param state: State of PRNG.\n",
    "    :type state: :class:`np.random.RandomState`\n",
    "    \n",
    "    Power decreases with -3 dB per octave.\n",
    "    Power density decreases with 6 dB per octave. \n",
    "    \"\"\"\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N%2\n",
    "    X = state.randn(N//2+1+uneven) + 1j * state.randn(N//2+1+uneven)\n",
    "    S = (np.arange(len(X))+1)# Filter\n",
    "    y = (irfft(X/S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)\n",
    "\n",
    "def violet_noise(N, state=None):\n",
    "    \"\"\"\n",
    "    Violet noise. Power increases with 6 dB per octave. \n",
    "    \n",
    "    :param N: Amount of samples.\n",
    "    :param state: State of PRNG.\n",
    "    :type state: :class:`np.random.RandomState`\n",
    "    \n",
    "    Power increases with +9 dB per octave.\n",
    "    Power density increases with +6 dB per octave. \n",
    "    \n",
    "    \"\"\"\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N%2\n",
    "    X = state.randn(N//2+1+uneven) + 1j * state.randn(N//2+1+uneven)\n",
    "    S = (np.arange(len(X)))# Filter\n",
    "    y = (irfft(X*S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf Utils\n",
    "def get_tensorflow_configuration(device=\"0\", memory_fraction=1):\n",
    "    \"\"\"\n",
    "    Function for selecting the GPU to use and the amount of memory the process is allowed to use\n",
    "    :param device: which device should be used (str)\n",
    "    :param memory_fraction: which proportion of memory must be allocated (float)\n",
    "    :return: config to be passed to the session (tf object)\n",
    "    \"\"\"\n",
    "    device = str(device)\n",
    "    config = tf.ConfigProto()\n",
    "    config.allow_soft_placement = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = memory_fraction\n",
    "    config.gpu_options.visible_device_list = device\n",
    "    return(config)\n",
    "\n",
    "\n",
    "def start_tensorflow_session(device=\"0\", memory_fraction=1):\n",
    "    \"\"\"\n",
    "    Starts a tensorflow session taking care of what GPU device is going to be used and\n",
    "    which is the fraction of memory that is going to be pre-allocated.\n",
    "    :device: string with the device number (str)\n",
    "    :memory_fraction: fraction of memory that is going to be pre-allocated in the specified\n",
    "    device (float [0, 1])\n",
    "    :return: configured tf.Session\n",
    "    \"\"\"\n",
    "    return(tf.Session(config=get_tensorflow_configuration(device=device, memory_fraction=memory_fraction)))\n",
    "\n",
    "\n",
    "def get_summary_writer(session, logs_path, project_id, version_id):\n",
    "    \"\"\"\n",
    "    For Tensorboard reporting\n",
    "    :param session: opened tensorflow session (tf.Session)\n",
    "    :param logs_path: path where tensorboard is looking for logs (str)\n",
    "    :param project_id: name of the project for reporting purposes (str)\n",
    "    :param version_id: name of the version for reporting purposes (str)\n",
    "    :return summary_writer: the tensorboard writer\n",
    "    \"\"\"\n",
    "    path = os.path.join(logs_path,\"{}_{}\".format(project_id, version_id)) \n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    summary_writer = tf.summary.FileWriter(path, graph_def=session.graph_def)\n",
    "    return(summary_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths management module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common paths\n",
    "def _norm_path(path):\n",
    "    \"\"\"\n",
    "    Decorator function intended for using it to normalize a the output of a path retrieval function. Useful for\n",
    "    fixing the slash/backslash windows cases.\n",
    "    \"\"\"\n",
    "    def normalize_path(*args, **kwargs):\n",
    "        return os.path.normpath(path(*args, **kwargs))\n",
    "    return normalize_path\n",
    "\n",
    "\n",
    "def _assure_path_exists(path):\n",
    "    \"\"\"\n",
    "    Decorator function intended for checking the existence of a the output of a path retrieval function. Useful for\n",
    "    fixing the slash/backslash windows cases.\n",
    "    \"\"\"\n",
    "    def assure_exists(*args, **kwargs):\n",
    "        p=path(*args, **kwargs)\n",
    "        assert os.path.exists(p), \"the following path does not exist: '{}'\".format(p)\n",
    "        return p\n",
    "    return assure_exists\n",
    "\n",
    "\n",
    "def _is_output_path(path):\n",
    "    \"\"\"\n",
    "    Decorator function intended for grouping the functions which are applied over the output of an output path retrieval\n",
    "    function\n",
    "    \"\"\"\n",
    "    @_norm_path\n",
    "    @_assure_path_exists\n",
    "    def check_existence_or_create_it(*args, **kwargs):\n",
    "        if not os.path.exists(path(*args, **kwargs)):\n",
    "            \"Path does not exist... creating it: {}\".format(path(*args, **kwargs))\n",
    "            os.makedirs(path(*args, **kwargs))\n",
    "        return path(*args, **kwargs)\n",
    "    return check_existence_or_create_it\n",
    "\n",
    "\n",
    "def _is_input_path(path):\n",
    "    \"\"\"\n",
    "    Decorator function intended for grouping the functions which are applied over the output of an input path retrieval\n",
    "    function\n",
    "    \"\"\"\n",
    "    @_norm_path\n",
    "    @_assure_path_exists\n",
    "    def check_existence(*args, **kwargs):\n",
    "        return path(*args, **kwargs)\n",
    "    return check_existence\n",
    "\n",
    "@_is_input_path\n",
    "def get_train_path():\n",
    "    path = r\"C:\\Users\\User\\ML\\data\\train\"\n",
    "    return path\n",
    "\n",
    "@_is_input_path\n",
    "def get_test_path():\n",
    "    path = r\"C:\\Users\\User\\ML\\data\\test\"\n",
    "    return path\n",
    "\n",
    "@_is_input_path\n",
    "def get_train_audio_path():\n",
    "    path = os.path.join(get_train_path(), \"audio\")\n",
    "    return path\n",
    "\n",
    "@_is_input_path\n",
    "def get_scoring_audio_path():\n",
    "    path = os.path.join(get_test_path(), \"audio\")\n",
    "    return path\n",
    "\n",
    "@_is_output_path\n",
    "def get_submissions_path():\n",
    "    path = r\"C:\\Users\\User\\ML\\data\"\n",
    "    return path\n",
    "\n",
    "@_is_output_path\n",
    "def get_silence_path():\n",
    "    path = r\"C:\\Users\\User\\ML\\data\"\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def batching(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data tools\n",
    "def read_wav(filepath, pad=True):\n",
    "    \"\"\"\n",
    "    Given the filepath of a wav file, this function reads it, normalizes it and pads\n",
    "    it to assure it has 16k samples.\n",
    "    :param filepath: existing filepath of a wav file (str)\n",
    "    :param pad: is padding required? (bool)\n",
    "    :returns: the sample and the target variable (tuple of (np.array, str))\n",
    "    \"\"\"\n",
    "    sample_rate, x = wavfile.read(filepath)\n",
    "    target = os.path.split(os.path.split(filepath)[0])[1]\n",
    "    assert sample_rate==16000\n",
    "    if pad:\n",
    "        return np.pad(x, (0, 16000-len(x)), mode=\"constant\")/32768, target\n",
    "    else:\n",
    "        return x/32768, target\n",
    "\n",
    "def get_batcher(list_of_paths, batch_size, label_encoder=None, scoring=False):\n",
    "    \"\"\"\n",
    "    Builds a batch generator given a list of batches\n",
    "    :param list_of_paths: list of tuples with elements of format (filepath, target) (list)\n",
    "    :param batch_size: size of the batch (int)\n",
    "    :param label_encoder: fitted LabelEncoder (sklearn.LabelEncoder|optional)\n",
    "    :param scoring: should the target be considered? (bool)\n",
    "    :returns: batch generator\n",
    "    \"\"\"\n",
    "    for filepaths in batching(list_of_paths, batch_size):\n",
    "        wavs, targets = zip(*list(map(read_wav, filepaths)))\n",
    "        if scoring:\n",
    "            yield np.expand_dims(np.row_stack(wavs), 2), filepaths\n",
    "        else:\n",
    "            if label_encoder is None:\n",
    "                yield np.expand_dims(np.row_stack(wavs), 2), np.row_stack(targets)\n",
    "            else:\n",
    "                yield np.expand_dims(np.row_stack(wavs), 2), np.expand_dims(label_encoder.transform(np.squeeze(targets)),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(object):\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.999, name=\"batch_norm\"):\n",
    "        with tf.variable_scope(name):\n",
    "            self.epsilon = epsilon\n",
    "            self.momentum = momentum\n",
    "            self.name = name\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        return tf.contrib.layers.batch_norm(x,\n",
    "                                            decay=self.momentum,\n",
    "                                            updates_collections=None,\n",
    "                                            epsilon=self.epsilon,\n",
    "                                            scale=True,\n",
    "                                            is_training=train,\n",
    "                                            scope=self.name)\n",
    "    \n",
    "    \n",
    "\n",
    "def inception_1d(x, is_train, depth, norm_function, activ_function, name):\n",
    "    \"\"\"\n",
    "    Inception 1D module implementation. \n",
    "    :param x: input to the current module (4D tensor with channels-last)\n",
    "    :param is_train: it is intented to be a boolean placeholder for controling the BatchNormalization behavior (0D tensor)\n",
    "    :param depth: linearly controls the depth of the network (int)\n",
    "    :param norm_function: normalization class (same format as the BatchNorm class above)\n",
    "    :param activ_function: tensorflow activation function (e.g. tf.nn.relu) \n",
    "    :param name: name of the variable scope (str)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        x_norm = norm_function(name=\"norm_input\")(x, train=is_train)\n",
    "\n",
    "        # Branch 1: 64 x conv 1x1 \n",
    "        branch_conv_1_1 = tf.layers.conv1d(inputs=x_norm, filters=16*depth, kernel_size=1,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_1_1\")\n",
    "        branch_conv_1_1 = norm_function(name=\"norm_conv_1_1\")(branch_conv_1_1, train=is_train)\n",
    "        branch_conv_1_1 = activ_function(branch_conv_1_1, \"activation_1_1\")\n",
    "\n",
    "        # Branch 2: 128 x conv 3x3 \n",
    "        branch_conv_3_3 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_3_3_1\")\n",
    "        branch_conv_3_3 = norm_function(name=\"norm_conv_3_3_1\")(branch_conv_3_3, train=is_train)\n",
    "        branch_conv_3_3 = activ_function(branch_conv_3_3, \"activation_3_3_1\")\n",
    "\n",
    "        branch_conv_3_3 = tf.layers.conv1d(inputs=branch_conv_3_3, filters=32*depth, kernel_size=3, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_3_3_2\")\n",
    "        branch_conv_3_3 = norm_function(name=\"norm_conv_3_3_2\")(branch_conv_3_3, train=is_train)\n",
    "        branch_conv_3_3 = activ_function(branch_conv_3_3, \"activation_3_3_2\")\n",
    "\n",
    "        # Branch 3: 128 x conv 5x5 \n",
    "        branch_conv_5_5 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_5_5_1\")\n",
    "        branch_conv_5_5 = norm_function(name=\"norm_conv_5_5_1\")(branch_conv_5_5, train=is_train)\n",
    "        branch_conv_5_5 = activ_function(branch_conv_5_5, \"activation_5_5_1\")\n",
    "\n",
    "        branch_conv_5_5 = tf.layers.conv1d(inputs=branch_conv_5_5, filters=32*depth, kernel_size=5, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_5_5_2\")\n",
    "        branch_conv_5_5 = norm_function(name=\"norm_conv_5_5_2\")(branch_conv_5_5, train=is_train)\n",
    "        branch_conv_5_5 = activ_function(branch_conv_5_5, \"activation_5_5_2\")\n",
    "\n",
    "        # Branch 4: 128 x conv 7x7\n",
    "        branch_conv_7_7 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_7_7_1\")\n",
    "        branch_conv_7_7 = norm_function(name=\"norm_conv_7_7_1\")(branch_conv_7_7, train=is_train)\n",
    "        branch_conv_7_7 = activ_function(branch_conv_7_7, \"activation_7_7_1\")\n",
    "\n",
    "        branch_conv_7_7 = tf.layers.conv1d(inputs=branch_conv_7_7, filters=32*depth, kernel_size=5, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_7_7_2\")\n",
    "        branch_conv_7_7 = norm_function(name=\"norm_conv_7_7_2\")(branch_conv_7_7, train=is_train)\n",
    "        branch_conv_7_7 = activ_function(branch_conv_7_7, \"activation_7_7_2\")\n",
    "\n",
    "        # Branch 5: 16 x (max_pool 3x3 + conv 1x1)\n",
    "        branch_maxpool_3_3 = tf.layers.max_pooling1d(inputs=x_norm, pool_size=3, strides=1, padding=\"same\", name=\"maxpool_3\")\n",
    "        branch_maxpool_3_3 = norm_function(name=\"norm_maxpool_3_3\")(branch_maxpool_3_3, train=is_train)\n",
    "        branch_maxpool_3_3 = tf.layers.conv1d(inputs=branch_maxpool_3_3, filters=16, kernel_size=1, \n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_maxpool_3\")\n",
    "\n",
    "        # Branch 6: 16 x (max_pool 5x5 + conv 1x1)\n",
    "        branch_maxpool_5_5 = tf.layers.max_pooling1d(inputs=x_norm, pool_size=5, strides=1, padding=\"same\", name=\"maxpool_5\")\n",
    "        branch_maxpool_5_5 = norm_function(name=\"norm_maxpool_5_5\")(branch_maxpool_5_5, train=is_train)\n",
    "        branch_maxpool_5_5 = tf.layers.conv1d(inputs=branch_maxpool_5_5, filters=16, kernel_size=1, \n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_maxpool_5\")\n",
    "\n",
    "        # Branch 7: 16 x (avg_pool 3x3 + conv 1x1)\n",
    "        branch_avgpool_3_3 = tf.layers.average_pooling1d(inputs=x_norm, pool_size=3, strides=1, padding=\"same\", name=\"avgpool_3\")\n",
    "        branch_avgpool_3_3 = norm_function(name=\"norm_avgpool_3_3\")(branch_avgpool_3_3, train=is_train)\n",
    "        branch_avgpool_3_3 = tf.layers.conv1d(inputs=branch_avgpool_3_3, filters=16, kernel_size=1,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_avgpool_3\")\n",
    "\n",
    "        # Branch 8: 16 x (avg_pool 5x5 + conv 1x1)\n",
    "        branch_avgpool_5_5 = tf.layers.average_pooling1d(inputs=x_norm, pool_size=5, strides=1, padding=\"same\", name=\"avgpool_5\")\n",
    "        branch_avgpool_5_5 = norm_function(name=\"norm_avgpool_5_5\")(branch_avgpool_5_5, train=is_train)\n",
    "        branch_avgpool_5_5 = tf.layers.conv1d(inputs=branch_avgpool_5_5, filters=16, kernel_size=1, \n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_avgpool_5\")\n",
    "\n",
    "        # Concatenate\n",
    "        output = tf.concat([branch_conv_1_1, branch_conv_3_3, branch_conv_5_5, branch_conv_7_7, branch_maxpool_3_3, \n",
    "                           branch_maxpool_5_5, branch_avgpool_3_3, branch_avgpool_5_5], axis=-1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-acd6d40bfedf>:10: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sample_rate, x = wavfile.read(filepath)\n"
     ]
    }
   ],
   "source": [
    "# Synthetic and provided noise addition\n",
    "filepaths_noise = glob.glob(os.path.join(get_train_audio_path(), \"_background_noise_\", \"*.wav\"))\n",
    "\n",
    "noise = np.concatenate(list(map(lambda x: read_wav(x, False)[0], filepaths_noise)))\n",
    "noise = np.concatenate([noise, noise[::-1]])\n",
    "synthetic_noise = np.concatenate([white_noise(N=16000*30, state=np.random.RandomState(655321)), \n",
    "                                  blue_noise(N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  pink_noise(N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  brown_noise(N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  violet_noise(N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  np.zeros(16000*60)])\n",
    "synthetic_noise /= np.max(np.abs(synthetic_noise))\n",
    "synthetic_noise = np.concatenate([synthetic_noise, (synthetic_noise+synthetic_noise[::-1])/2])\n",
    "all_noise = np.concatenate([noise, synthetic_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████████████████████████████████████▉                                       | 3886/8000 [00:02<00:02, 1657.98it/s]<ipython-input-9-da4d15b20213>:17: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ((32767*clip/np.max(np.abs(clip))).astype(np.int16)))\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:04<00:00, 1608.50it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(655321)\n",
    "random.seed(655321)\n",
    "\n",
    "path = get_silence_path()\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path) # It fails in kaggle kernel due to the read-only filesystem\n",
    "\n",
    "for noise_clip_no in tqdm(range(8000)):\n",
    "    if noise_clip_no<=4000:\n",
    "        idx = np.random.randint(0, len(noise)-16000)\n",
    "        clip = noise[idx:(idx+16000)]\n",
    "    else:\n",
    "        idx = np.random.randint(0, len(synthetic_noise)-16000)\n",
    "        clip = synthetic_noise[idx:(idx+16000)]\n",
    "    wavfile.write(os.path.join(path, \"{0:04d}.wav\".format(noise_clip_no)), 16000, \n",
    "                               ((32767*clip/np.max(np.abs(clip))).astype(np.int16)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = glob.glob(os.path.join(get_train_audio_path(), \"**/*.wav\"), recursive=True)\n",
    "filepaths += glob.glob(os.path.join(get_silence_path(), \"**/*.wav\"), recursive=True)\n",
    "filepaths = list(filter(lambda fp: \"_background_noise_\" not in fp, filepaths))\n",
    "validation_list = open(os.path.join(get_train_path(), \"validation_list.txt\")).readlines()\n",
    "test_list = open(os.path.join(get_train_path(), \"testing_list.txt\")).readlines()\n",
    "validation_list = list(map(lambda fn: os.path.join(get_train_audio_path(), fn.strip()), validation_list))\n",
    "testing_list = list(map(lambda fn: os.path.join(get_train_audio_path(), fn.strip()), test_list))\n",
    "training_list = np.setdiff1d(filepaths, validation_list+testing_list).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(655321)\n",
    "random.shuffle(filepaths)\n",
    "random.shuffle(validation_list)\n",
    "random.shuffle(testing_list)\n",
    "random.shuffle(training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Unit-Tests\n",
    "# Test number of files and their consistencies\n",
    "assert all(map(lambda fp: os.path.splitext(fp)[1]==\".wav\", filepaths))\n",
    "#assert len(filepaths)==64727 - 6 + 8000\n",
    "#assert len(training_list) == len(filepaths) - 6798 - 6835 \n",
    "assert len(validation_list) == 6798\n",
    "assert len(testing_list) == 6835\n",
    "\n",
    "# Test file existence\n",
    "assert all(map(lambda fn: os.path.exists(os.path.join(fn)), validation_list))\n",
    "assert all(map(lambda fn: os.path.exists(os.path.join(fn)), testing_list))\n",
    "assert all(map(lambda fn: os.path.exists(os.path.join(fn)), training_list))\n",
    "#assert set(validation_list + testing_list + training_list) == set(filepaths)\n",
    "\n",
    "# Test non-overlap among sets\n",
    "assert len(np.intersect1d(validation_list, testing_list))==0\n",
    "assert len(np.intersect1d(training_list, testing_list))==0\n",
    "assert len(np.intersect1d(training_list, validation_list))==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'audio': 158538,\n",
       "         'one': 4740,\n",
       "         'zero': 4752,\n",
       "         'four': 4744,\n",
       "         'nine': 4728,\n",
       "         'house': 3500,\n",
       "         'off': 4714,\n",
       "         'bed': 3426,\n",
       "         'eight': 4704,\n",
       "         'down': 4718,\n",
       "         'go': 4744,\n",
       "         'up': 4750,\n",
       "         'left': 4706,\n",
       "         'five': 4714,\n",
       "         'data': 8000,\n",
       "         'bird': 3462,\n",
       "         'two': 4746,\n",
       "         'no': 4750,\n",
       "         'wow': 3490,\n",
       "         'yes': 4754,\n",
       "         'tree': 3466,\n",
       "         'dog': 3492,\n",
       "         'three': 4712,\n",
       "         'on': 4734,\n",
       "         'six': 4738,\n",
       "         'sheila': 3468,\n",
       "         'happy': 3484,\n",
       "         'seven': 4754,\n",
       "         'cat': 3466,\n",
       "         'stop': 4760,\n",
       "         'right': 4734,\n",
       "         'marvin': 3492})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classes processing\n",
    "cardinal_classes = list(set(map(lambda fp:os.path.split(os.path.split(fp)[0])[1], filepaths)))\n",
    "le_classes = LabelEncoder().fit(cardinal_classes)\n",
    "Counter(map(lambda fp:os.path.split(os.path.split(fp)[0])[1], filepaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Unit-Tests\n",
    "# Test data preparation\n",
    "_gen_test = get_batcher(filepaths, 1000)\n",
    "batch_a_wav, batch_a_target = next(_gen_test)\n",
    "batch_b_wav, batch_b_target = next(_gen_test)\n",
    "_gen_test_le = get_batcher(filepaths, 1000, label_encoder=le_classes)\n",
    "batch_le_wav, batch_le_target = next(_gen_test_le)\n",
    "\n",
    "# Test batch matrix shape coherences\n",
    "assert batch_a_wav.shape == (1000, 16000, 1)\n",
    "assert batch_le_wav.shape == (1000, 16000, 1)\n",
    "assert batch_a_wav.shape == batch_b_wav.shape == batch_le_wav.shape\n",
    "\n",
    "# Test batch reproducibility\n",
    "assert np.sum(np.abs(batch_a_wav-batch_b_wav)) != 0\n",
    "assert len(batch_a_target) == len(batch_b_target) == len(batch_le_target)\n",
    "assert any(batch_a_target != batch_b_target)\n",
    "\n",
    "# Test classes label encoder\n",
    "assert all(batch_le_target == np.expand_dims(le_classes.transform(np.squeeze(batch_a_target)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NameSpacer:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "class Architecture:\n",
    "    def __init__(self, class_cardinality, seq_len=16000, name=\"architecture\"):\n",
    "        self.seq_len = seq_len\n",
    "        self.class_cardinality = class_cardinality\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.name=name\n",
    "        self.define_computation_graph()\n",
    "        \n",
    "        #Aliases\n",
    "        self.ph = self.placeholders\n",
    "        self.op = self.optimizers\n",
    "        self.summ = self.summaries\n",
    "\n",
    "    def define_computation_graph(self):\n",
    "        # Reset graph\n",
    "        tf.reset_default_graph()\n",
    "        self.placeholders = NameSpacer(**self.define_placeholders())\n",
    "        self.core_model = NameSpacer(**self.define_core_model())\n",
    "        self.losses = NameSpacer(**self.define_losses())\n",
    "        self.optimizers = NameSpacer(**self.define_optimizers())\n",
    "        self.summaries = NameSpacer(**self.define_summaries())\n",
    "\n",
    "    def define_placeholders(self):\n",
    "        with tf.variable_scope(\"Placeholders\"):\n",
    "            wav_in = tf.placeholder(dtype=tf.float32, shape=(None, self.seq_len, 1), name=\"wav_in\")\n",
    "            is_train = tf.placeholder(dtype=tf.bool, shape=None, name=\"is_train\")\n",
    "            target = tf.placeholder(dtype=tf.int32, shape=(None, 1), name=\"target\")\n",
    "            acc_dev = tf.placeholder(dtype=tf.float32, shape=None, name=\"acc_dev\")\n",
    "            loss_dev = tf.placeholder(dtype=tf.float32, shape=None, name=\"loss_dev\")\n",
    "            return({\"wav_in\": wav_in, \"target\": target, \"is_train\": is_train, \"acc_dev\": \n",
    "                    acc_dev, \"loss_dev\": loss_dev})\n",
    "        \n",
    "    def define_core_model(self):\n",
    "        with tf.variable_scope(\"Core_Model\"):\n",
    "            x = inception_1d(x=self.placeholders.wav_in, is_train=self.placeholders.is_train, \n",
    "                             norm_function=BatchNorm, activ_function=tf.nn.relu, depth=1,\n",
    "                             name=\"Inception_1_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=1, name=\"Inception_1_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=1, name=\"Inception_2_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=1, name=\"Inception_2_3\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_2\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_3_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_3_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_3\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_4_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_4_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_4\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_5_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_5_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_5\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_6_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_6_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_6\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_7_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_7_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_7\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_8_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_8_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_8\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_9_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_9_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_9\")\n",
    "            x = tf.contrib.layers.flatten(x)\n",
    "            x = tf.layers.dense(BatchNorm(name=\"bn_dense_1\")(x,train=self.placeholders.is_train),\n",
    "                                128, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"dense_1\")\n",
    "            output = tf.layers.dense(BatchNorm(name=\"bn_dense_2\")(x,train=self.placeholders.is_train),\n",
    "                                self.class_cardinality, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"output\")\n",
    "            return({\"output\": output})\n",
    "        \n",
    "    def define_losses(self):\n",
    "        with tf.variable_scope(\"Losses\"):\n",
    "            softmax_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.squeeze(self.placeholders.target), \n",
    "                                                                        logits=self.core_model.output,\n",
    "                                                                        name=\"softmax\")\n",
    "            return({\"softmax\": softmax_ce})\n",
    "\n",
    "    def define_optimizers(self):\n",
    "        with tf.variable_scope(\"Optimization\"):\n",
    "            op = self.optimizer.minimize(self.losses.softmax)\n",
    "            return({\"op\": op})\n",
    "\n",
    "    def define_summaries(self):\n",
    "        with tf.variable_scope(\"Summaries\"):\n",
    "            ind_max = tf.squeeze(tf.cast(tf.argmax(self.core_model.output, axis=1), tf.int32))\n",
    "            target = tf.squeeze(self.placeholders.target)\n",
    "            acc= tf.reduce_mean(tf.cast(tf.equal(ind_max, target), tf.float32))\n",
    "            loss = tf.reduce_mean(self.losses.softmax)\n",
    "            train_scalar_probes = {\"accuracy\": acc, \n",
    "                                   \"loss\": loss}\n",
    "            train_performance_scalar = [tf.summary.scalar(k, tf.reduce_mean(v), family=self.name) \n",
    "                                        for k, v in train_scalar_probes.items()]\n",
    "            train_performance_scalar = tf.summary.merge(train_performance_scalar)\n",
    "\n",
    "            dev_scalar_probes = {\"acc_dev\": self.placeholders.acc_dev, \n",
    "                                 \"loss_dev\": self.placeholders.loss_dev}\n",
    "            dev_performance_scalar = [tf.summary.scalar(k, v, family=self.name) for k, v in dev_scalar_probes.items()]\n",
    "            dev_performance_scalar = tf.summary.merge(dev_performance_scalar)\n",
    "            return({\"accuracy\": acc, \"loss\": loss, \"s_tr\": train_performance_scalar, \"s_de\": dev_performance_scalar})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v1' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-00128af58348>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArchitecture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcardinal_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"wavception\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-370ad97086ad>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, class_cardinality, seq_len, name)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_computation_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#Aliases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-370ad97086ad>\u001b[0m in \u001b[0;36mdefine_computation_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNameSpacer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_placeholders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNameSpacer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_core_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNameSpacer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_losses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNameSpacer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_optimizers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-370ad97086ad>\u001b[0m in \u001b[0;36mdefine_core_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdefine_core_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Core_Model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             x = inception_1d(x=self.placeholders.wav_in, is_train=self.placeholders.is_train, \n\u001b[0m\u001b[0;32m     40\u001b[0m                              \u001b[0mnorm_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBatchNorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactiv_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                              name=\"Inception_1_1\")\n",
      "\u001b[1;32m<ipython-input-7-30e01ff3ba98>\u001b[0m in \u001b[0;36minception_1d\u001b[1;34m(x, is_train, depth, norm_function, activ_function, name)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \"\"\"\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mx_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"norm_input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# Branch 1: 64 x conv 1x1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-30e01ff3ba98>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, train)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         return tf.contrib.layers.batch_norm(x,\n\u001b[0m\u001b[0;32m     10\u001b[0m                                             \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                             \u001b[0mupdates_collections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.compat.v1' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "net = Architecture(class_cardinality=len(cardinal_classes), name=\"wavception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n"
     ]
    }
   ],
   "source": [
    "sess = start_tensorflow_session(device=\"1\")\n",
    "sw = get_summary_writer(sess, \"~/.logs_tensorboard/\", \"wavception\", \"V1\") # Adjust your tensorboard logs path here\n",
    "c=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(655321)\n",
    "random.seed(655321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-59f035617ed5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbatcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mle_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         _, loss, acc, s = sess.run([net.op.op, net.losses.softmax, net.summ.accuracy, net.summ.s_tr],\n\u001b[0m\u001b[0;32m      6\u001b[0m                                  feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y, \n\u001b[0;32m      7\u001b[0m                                             net.ph.is_train: True})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(50000):\n",
    "    random.shuffle(training_list)\n",
    "    batcher = get_batcher(training_list, 16, le_classes)\n",
    "    for i, (batch_x, batch_y) in enumerate(batcher):\n",
    "        _, loss, acc, s = sess.run([net.op.op, net.losses.softmax, net.summ.accuracy, net.summ.s_tr],\n",
    "                                 feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y, \n",
    "                                            net.ph.is_train: True})\n",
    "        print(\"[{0:04d}|{1:04d}] Accuracy train: {2:.2f}%\".format(epoch, i, acc*100))\n",
    "        sw.add_summary(s, c)\n",
    "        \n",
    "        if c%1000==0: # Validation\n",
    "            accuracies_dev=[]\n",
    "            losses_dev=[]\n",
    "            batcher = get_batcher(validation_list, 16, le_classes)\n",
    "            for i, (batch_x, batch_y) in enumerate(batcher):\n",
    "                acc, loss= sess.run([net.summ.accuracy, net.summ.loss], \n",
    "                               feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y, \n",
    "                                          net.ph.is_train: False})\n",
    "                accuracies_dev.append(acc)\n",
    "                losses_dev.append(loss)\n",
    "            s = sess.run(net.summ.s_de, feed_dict={net.ph.acc_dev: np.mean(accuracies_dev),\n",
    "                                                        net.ph.loss_dev: np.mean(losses_dev)})\n",
    "            sw.add_summary(s, c)\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-d70187737f17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mle_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     acc= sess.run(net.summ.accuracy, feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y, \n\u001b[0m\u001b[0;32m      5\u001b[0m                                                      net.ph.is_train: False})\n\u001b[0;32m      6\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "accuracies=[]\n",
    "batcher = get_batcher(testing_list, 64, le_classes)\n",
    "for i, (batch_x, batch_y) in tqdm(enumerate(batcher)):\n",
    "    acc= sess.run(net.summ.accuracy, feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y, \n",
    "                                                     net.ph.is_train: False})\n",
    "    accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and submission building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_list = glob.glob(os.path.join(get_scoring_audio_path(), \"*.wav\"), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher = get_batcher(scoring_list, 80, le_classes, scoring=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-043346b5721c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepaths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwav_in\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mfns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "fns = []\n",
    "prds = []\n",
    "for i, (batch_x, filepaths) in tqdm(enumerate(batcher)):\n",
    "    pred = sess.run(net.core_model.output, feed_dict={net.ph.wav_in: batch_x, net.ph.is_train: False})\n",
    "    fns.extend(map(lambda f:os.path.split(f)[1], filepaths))\n",
    "    prds.extend(map(lambda f:np.argmax(pred, axis=1).tolist(), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission storage\n",
    "df=pd.DataFrame({\"fname\":fns, \"label\": prds})\n",
    "df.label = le_classes.inverse_transform(df.label)\n",
    "df.loc[~df.label.isin([\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"silence\"]), \"label\"]=\"unknown\"\n",
    "df.to_csv(os.path.join(get_submissions_path(), \"submission.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
