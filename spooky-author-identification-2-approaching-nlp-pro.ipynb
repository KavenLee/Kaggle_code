{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM,GRU\nfrom keras.layers.core import Dense, Activation,Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras. callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk. corpus import stopwords\nstop_words=stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:49:05.524683Z","iopub.execute_input":"2021-07-24T06:49:05.525147Z","iopub.status.idle":"2021-07-24T06:49:13.064069Z","shell.execute_reply.started":"2021-07-24T06:49:05.525053Z","shell.execute_reply":"2021-07-24T06:49:13.062955Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('../input/spooky/train.csv')\ntest=pd.read_csv('../input/spooky/test.csv')\nsample=pd.read_csv('../input/spooky/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:49:13.065957Z","iopub.execute_input":"2021-07-24T06:49:13.066379Z","iopub.status.idle":"2021-07-24T06:49:13.244136Z","shell.execute_reply.started":"2021-07-24T06:49:13.066336Z","shell.execute_reply":"2021-07-24T06:49:13.243003Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:49:13.247006Z","iopub.execute_input":"2021-07-24T06:49:13.247495Z","iopub.status.idle":"2021-07-24T06:49:13.278827Z","shell.execute_reply.started":"2021-07-24T06:49:13.247451Z","shell.execute_reply":"2021-07-24T06:49:13.277526Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"        id                                               text author\n0  id26305  This process, however, afforded me no means of...    EAP\n1  id17569  It never once occurred to me that the fumbling...    HPL\n2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n3  id27763  How lovely is spring As we looked from Windsor...    MWS\n4  id12958  Finding nothing else, not even gold, the Super...    HPL","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id26305</td>\n      <td>This process, however, afforded me no means of...</td>\n      <td>EAP</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id17569</td>\n      <td>It never once occurred to me that the fumbling...</td>\n      <td>HPL</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id11008</td>\n      <td>In his left hand was a gold snuff box, from wh...</td>\n      <td>EAP</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id27763</td>\n      <td>How lovely is spring As we looked from Windsor...</td>\n      <td>MWS</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id12958</td>\n      <td>Finding nothing else, not even gold, the Super...</td>\n      <td>HPL</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:49:13.281013Z","iopub.execute_input":"2021-07-24T06:49:13.281471Z","iopub.status.idle":"2021-07-24T06:49:13.295761Z","shell.execute_reply.started":"2021-07-24T06:49:13.281430Z","shell.execute_reply":"2021-07-24T06:49:13.294149Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"        id                                               text\n0  id02310  Still, as I urged our leaving Ireland with suc...\n1  id24541  If a fire wanted fanning, it could readily be ...\n2  id00134  And when they had broken down the frail door t...\n3  id27757  While I was thinking how I should possibly man...\n4  id04081  I am not sure to what limit his knowledge may ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id02310</td>\n      <td>Still, as I urged our leaving Ireland with suc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id24541</td>\n      <td>If a fire wanted fanning, it could readily be ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id00134</td>\n      <td>And when they had broken down the frail door t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id27757</td>\n      <td>While I was thinking how I should possibly man...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id04081</td>\n      <td>I am not sure to what limit his knowledge may ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sample.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:49:13.298077Z","iopub.execute_input":"2021-07-24T06:49:13.298653Z","iopub.status.idle":"2021-07-24T06:49:13.316033Z","shell.execute_reply.started":"2021-07-24T06:49:13.298591Z","shell.execute_reply":"2021-07-24T06:49:13.314688Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        id       EAP       HPL       MWS\n0  id02310  0.403494  0.287808  0.308698\n1  id24541  0.403494  0.287808  0.308698\n2  id00134  0.403494  0.287808  0.308698\n3  id27757  0.403494  0.287808  0.308698\n4  id04081  0.403494  0.287808  0.308698","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>EAP</th>\n      <th>HPL</th>\n      <th>MWS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id02310</td>\n      <td>0.403494</td>\n      <td>0.287808</td>\n      <td>0.308698</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id24541</td>\n      <td>0.403494</td>\n      <td>0.287808</td>\n      <td>0.308698</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id00134</td>\n      <td>0.403494</td>\n      <td>0.287808</td>\n      <td>0.308698</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id27757</td>\n      <td>0.403494</td>\n      <td>0.287808</td>\n      <td>0.308698</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id04081</td>\n      <td>0.403494</td>\n      <td>0.287808</td>\n      <td>0.308698</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"\n    Multi class version of Logarithmic Loss metrinc.\n    :param actual : Array containing the actual target classes\n    :param predicted : Matrix with class predictions, one probability per class\n    \"\"\"\n    \n    # Convert 'actual' to a binary array if it;s not already:\n    if len(actual.shape)==1:\n        actual2=np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[1,val]=1\n        actual=actual2\n        \n    clip=np.clip(predicted,eps,1-eps)\n    rows=actual.shape[0]\n    vsota = np.sum(actual*np.log(clip))\n    return -1.0/rows*vsota","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:49:13.318103Z","iopub.execute_input":"2021-07-24T06:49:13.318650Z","iopub.status.idle":"2021-07-24T06:49:13.328820Z","shell.execute_reply.started":"2021-07-24T06:49:13.318604Z","shell.execute_reply":"2021-07-24T06:49:13.327304Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.author.values)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:49:13.330500Z","iopub.execute_input":"2021-07-24T06:49:13.331235Z","iopub.status.idle":"2021-07-24T06:49:13.349297Z","shell.execute_reply.started":"2021-07-24T06:49:13.331178Z","shell.execute_reply":"2021-07-24T06:49:13.348072Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"xtrain,xvalid,ytrain,yvalid = train_test_split(train.text.values, y, \n                                              stratify=y,\n                                              random_state=42,\n                                              test_size=0.1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:49:13.352715Z","iopub.execute_input":"2021-07-24T06:49:13.353183Z","iopub.status.idle":"2021-07-24T06:49:13.385559Z","shell.execute_reply.started":"2021-07-24T06:49:13.353137Z","shell.execute_reply":"2021-07-24T06:49:13.384499Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(xtrain.shape)\nprint(xvalid.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:49:13.387156Z","iopub.execute_input":"2021-07-24T06:49:13.387624Z","iopub.status.idle":"2021-07-24T06:49:13.394270Z","shell.execute_reply.started":"2021-07-24T06:49:13.387579Z","shell.execute_reply":"2021-07-24T06:49:13.392971Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(17621,)\n(1958,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Building Basic Models","metadata":{}},{"cell_type":"code","source":"# Always start with these features. They work (almost) everytime!\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv =  tfv.transform(xtrain) \nxvalid_tfv = tfv.transform(xvalid)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:49:13.395932Z","iopub.execute_input":"2021-07-24T06:49:13.396639Z","iopub.status.idle":"2021-07-24T06:49:15.987885Z","shell.execute_reply.started":"2021-07-24T06:49:13.396593Z","shell.execute_reply":"2021-07-24T06:49:15.986796Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C=1.0,solver='liblinear')\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:49:39.630640Z","iopub.execute_input":"2021-07-24T06:49:39.631043Z","iopub.status.idle":"2021-07-24T06:49:39.826289Z","shell.execute_reply.started":"2021-07-24T06:49:39.631008Z","shell.execute_reply":"2021-07-24T06:49:39.825072Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"logloss: 0.002 \n","output_type":"stream"}]},{"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',ngram_range=(1,3),\n                     stop_words='english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain)+list(xvalid))\nxtrain_ctv = ctv.transform(xtrain)\nxvalid_ctv = ctv.transform(xvalid)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:49:47.607046Z","iopub.execute_input":"2021-07-24T06:49:47.607569Z","iopub.status.idle":"2021-07-24T06:49:51.297339Z","shell.execute_reply.started":"2021-07-24T06:49:47.607500Z","shell.execute_reply":"2021-07-24T06:49:51.296218Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0,solver='liblinear')\nclf.fit(xtrain_ctv,ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint('logloss : %0.3f ' % multiclass_logloss(yvalid,predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:50:33.011247Z","iopub.execute_input":"2021-07-24T06:50:33.011645Z","iopub.status.idle":"2021-07-24T06:50:34.239536Z","shell.execute_reply.started":"2021-07-24T06:50:33.011613Z","shell.execute_reply":"2021-07-24T06:50:34.238295Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"logloss : 0.003 \n","output_type":"stream"}]},{"cell_type":"code","source":"# Fitting a simple Naive Bayes on TFIDF\nclf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:50:38.238962Z","iopub.execute_input":"2021-07-24T06:50:38.239363Z","iopub.status.idle":"2021-07-24T06:50:38.258976Z","shell.execute_reply.started":"2021-07-24T06:50:38.239331Z","shell.execute_reply":"2021-07-24T06:50:38.257443Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"logloss: 0.002 \n","output_type":"stream"}]},{"cell_type":"code","source":"# Fitting a simple Naive Bayes on Counts\nclf = MultinomialNB()\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:50:39.655508Z","iopub.execute_input":"2021-07-24T06:50:39.655974Z","iopub.status.idle":"2021-07-24T06:50:39.706065Z","shell.execute_reply.started":"2021-07-24T06:50:39.655942Z","shell.execute_reply":"2021-07-24T06:50:39.704608Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"logloss: 0.003 \n","output_type":"stream"}]},{"cell_type":"code","source":"# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\nsvd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:50:43.186974Z","iopub.execute_input":"2021-07-24T06:50:43.187420Z","iopub.status.idle":"2021-07-24T06:50:44.617347Z","shell.execute_reply.started":"2021-07-24T06:50:43.187379Z","shell.execute_reply":"2021-07-24T06:50:44.616093Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple SVM\nclf = SVC(C=1.0, probability=True) # since we need probabilities\nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict_proba(xvalid_svd_scl)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:50:47.824826Z","iopub.execute_input":"2021-07-24T06:50:47.825206Z","iopub.status.idle":"2021-07-24T06:55:29.517960Z","shell.execute_reply.started":"2021-07-24T06:50:47.825173Z","shell.execute_reply":"2021-07-24T06:55:29.516682Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"logloss: 0.002 \n","output_type":"stream"}]},{"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_tfv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_tfv.tocsc())\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:55:33.454295Z","iopub.execute_input":"2021-07-24T06:55:33.454717Z","iopub.status.idle":"2021-07-24T06:56:27.925569Z","shell.execute_reply.started":"2021-07-24T06:55:33.454678Z","shell.execute_reply":"2021-07-24T06:56:27.924346Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[06:55:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nlogloss: 0.002 \n","output_type":"stream"}]},{"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_ctv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_ctv.tocsc())\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:56:35.524276Z","iopub.execute_input":"2021-07-24T06:56:35.524687Z","iopub.status.idle":"2021-07-24T07:00:30.954496Z","shell.execute_reply.started":"2021-07-24T06:56:35.524653Z","shell.execute_reply":"2021-07-24T07:00:30.952224Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"[06:56:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nlogloss: 0.002 \n","output_type":"stream"}]},{"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf svd features\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:02:12.458124Z","iopub.execute_input":"2021-07-24T07:02:12.458616Z","iopub.status.idle":"2021-07-24T07:06:24.688007Z","shell.execute_reply.started":"2021-07-24T07:02:12.458540Z","shell.execute_reply":"2021-07-24T07:06:24.686869Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[07:02:13] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nlogloss: 0.002 \n","output_type":"stream"}]},{"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf svd features\nclf = xgb.XGBClassifier(nthread=10)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:06:29.575158Z","iopub.execute_input":"2021-07-24T07:06:29.575579Z","iopub.status.idle":"2021-07-24T07:09:05.319219Z","shell.execute_reply.started":"2021-07-24T07:06:29.575525Z","shell.execute_reply":"2021-07-24T07:09:05.318172Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[07:06:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nlogloss: 0.002 \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Grid Search","metadata":{}},{"cell_type":"code","source":"mll_scorer = metrics.make_scorer(multiclass_logloss,greater_is_better=False, needs_proba=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:09:08.902808Z","iopub.execute_input":"2021-07-24T07:09:08.903248Z","iopub.status.idle":"2021-07-24T07:09:08.908764Z","shell.execute_reply.started":"2021-07-24T07:09:08.903204Z","shell.execute_reply":"2021-07-24T07:09:08.907604Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Initialize SVD\nsvd = TruncatedSVD()\n\n# Initialize the standard scaler\nscl = preprocessing.StandardScaler()\n\n# We will use logistic regression here..\nlr_model = LogisticRegression()\n\n# Create the pipeline\nclf = pipeline.Pipeline([('svd',svd),\n                        ('scl',scl),\n                        ('lr',lr_model)])","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:09:11.483418Z","iopub.execute_input":"2021-07-24T07:09:11.483814Z","iopub.status.idle":"2021-07-24T07:09:11.490206Z","shell.execute_reply.started":"2021-07-24T07:09:11.483784Z","shell.execute_reply":"2021-07-24T07:09:11.488722Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"param_grid = {'svd__n_components' : [120,180],\n             'lr__C':[0.1,1.0,10],\n             'lr__penalty':['l1','l2']}","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:09:13.240376Z","iopub.execute_input":"2021-07-24T07:09:13.240792Z","iopub.status.idle":"2021-07-24T07:09:13.246974Z","shell.execute_reply.started":"2021-07-24T07:09:13.240760Z","shell.execute_reply":"2021-07-24T07:09:13.244933Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf,param_grid=param_grid,scoring=mll_scorer,\n                    verbose=10, n_jobs=-1,  refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(xtrain_tfv,ytrain) # we can use the full data here but im only using xtrain\nprint('Best score : %0.3f' % model.best_score_)\nprint('Best parameters set : ')\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print('\\t%s : %r' %(param_name, best_parameters[param_name]))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:09:15.063191Z","iopub.execute_input":"2021-07-24T07:09:15.063600Z","iopub.status.idle":"2021-07-24T07:09:47.719294Z","shell.execute_reply.started":"2021-07-24T07:09:15.063543Z","shell.execute_reply":"2021-07-24T07:09:47.717984Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 12 candidates, totalling 24 fits\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    3.8s\n[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    6.7s\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   12.9s\n[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   17.5s\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   27.3s\n[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   30.4s remaining:    0.0s\n[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   30.4s finished\n","output_type":"stream"},{"name":"stdout","text":"Best score : -0.000\nBest parameters set : \n\tlr__C : 0.1\n\tlr__penalty : 'l2'\n\tsvd__n_components : 120\n","output_type":"stream"}]},{"cell_type":"code","source":"nb_model = MultinomialNB()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('nb', nb_model)])\n\n# parameter grid\nparam_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n                                 verbose=10, n_jobs=1, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain. \nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:09:57.248330Z","iopub.execute_input":"2021-07-24T07:09:57.248844Z","iopub.status.idle":"2021-07-24T07:09:57.507326Z","shell.execute_reply.started":"2021-07-24T07:09:57.248811Z","shell.execute_reply":"2021-07-24T07:09:57.506230Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 6 candidates, totalling 12 fits\n[CV] nb__alpha=0.001 .................................................\n[CV] .................... nb__alpha=0.001, score=-0.001, total=   0.0s\n[CV] nb__alpha=0.001 .................................................\n[CV] .................... nb__alpha=0.001, score=-0.001, total=   0.0s\n[CV] nb__alpha=0.01 ..................................................\n[CV] ..................... nb__alpha=0.01, score=-0.001, total=   0.0s\n[CV] nb__alpha=0.01 ..................................................\n[CV] ..................... nb__alpha=0.01, score=-0.001, total=   0.0s\n[CV] nb__alpha=0.1 ...................................................\n[CV] ...................... nb__alpha=0.1, score=-0.001, total=   0.0s\n[CV] nb__alpha=0.1 ...................................................\n[CV] ...................... nb__alpha=0.1, score=-0.001, total=   0.0s\n[CV] nb__alpha=1 .....................................................\n[CV] ........................ nb__alpha=1, score=-0.000, total=   0.0s\n[CV] nb__alpha=1 .....................................................\n[CV] ........................ nb__alpha=1, score=-0.000, total=   0.0s\n[CV] nb__alpha=10 ....................................................\n[CV] ....................... nb__alpha=10, score=-0.000, total=   0.0s\n[CV] nb__alpha=10 ....................................................\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.1s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.2s remaining:    0.0s\n","output_type":"stream"},{"name":"stdout","text":"[CV] ....................... nb__alpha=10, score=-0.000, total=   0.0s\n[CV] nb__alpha=100 ...................................................\n[CV] ...................... nb__alpha=100, score=-0.000, total=   0.0s\n[CV] nb__alpha=100 ...................................................\n[CV] ...................... nb__alpha=100, score=-0.000, total=   0.0s\nBest score: -0.000\nBest parameters set:\n\tnb__alpha: 100\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.2s finished\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Word Vectors","metadata":{}},{"cell_type":"code","source":"# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('../input/glove840b300dtxt/glove.840B.300d.txt','rt',encoding='utf8')\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:10:23.418946Z","iopub.execute_input":"2021-07-24T07:10:23.419380Z","iopub.status.idle":"2021-07-24T07:10:30.421147Z","shell.execute_reply.started":"2021-07-24T07:10:23.419343Z","shell.execute_reply":"2021-07-24T07:10:30.419362Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"52343it [00:06, 7527.46it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-f71719d8f319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: could not convert string to float: '.'"],"ename":"ValueError","evalue":"could not convert string to float: '.'","output_type":"error"}]},{"cell_type":"code","source":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v ** 2).sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:10:36.665876Z","iopub.execute_input":"2021-07-24T07:10:36.666403Z","iopub.status.idle":"2021-07-24T07:10:36.681460Z","shell.execute_reply.started":"2021-07-24T07:10:36.666355Z","shell.execute_reply":"2021-07-24T07:10:36.679719Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# create sentence vectors using the above function for training and validation set\nxtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\nxvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:10:39.959893Z","iopub.execute_input":"2021-07-24T07:10:39.960273Z","iopub.status.idle":"2021-07-24T07:10:49.632169Z","shell.execute_reply.started":"2021-07-24T07:10:39.960241Z","shell.execute_reply":"2021-07-24T07:10:49.630816Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"100%|██████████| 17621/17621 [00:08<00:00, 2020.12it/s]\n100%|██████████| 1958/1958 [00:00<00:00, 2088.67it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"xtrain_glove = np.array(xtrain_glove)\nxvalid_glove = np.array(xvalid_glove)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:10:54.663985Z","iopub.execute_input":"2021-07-24T07:10:54.664347Z","iopub.status.idle":"2021-07-24T07:10:54.705764Z","shell.execute_reply.started":"2021-07-24T07:10:54.664317Z","shell.execute_reply":"2021-07-24T07:10:54.704603Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(nthread=10, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:11:00.180449Z","iopub.execute_input":"2021-07-24T07:11:00.180864Z","iopub.status.idle":"2021-07-24T07:17:26.589408Z","shell.execute_reply.started":"2021-07-24T07:11:00.180834Z","shell.execute_reply":"2021-07-24T07:17:26.587162Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[07:11:00] WARNING: ../src/learner.cc:573: \nParameters: { \"silent\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[07:11:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nlogloss: 0.002 \n","output_type":"stream"}]},{"cell_type":"code","source":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:17:49.393229Z","iopub.execute_input":"2021-07-24T07:17:49.393612Z","iopub.status.idle":"2021-07-24T07:27:59.790805Z","shell.execute_reply.started":"2021-07-24T07:17:49.393575Z","shell.execute_reply":"2021-07-24T07:27:59.789521Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"[07:17:49] WARNING: ../src/learner.cc:573: \nParameters: { \"silent\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[07:17:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nlogloss: 0.002 \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Deep Learnning","metadata":{}},{"cell_type":"code","source":"# scale the data before any neural net:\nscl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.transform(xvalid_glove)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:28:08.983050Z","iopub.execute_input":"2021-07-24T07:28:08.983490Z","iopub.status.idle":"2021-07-24T07:28:09.114256Z","shell.execute_reply.started":"2021-07-24T07:28:08.983460Z","shell.execute_reply":"2021-07-24T07:28:09.112962Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# we need to binarize the labels for the neural net\nytrain_enc = np_utils.to_categorical(ytrain)\nyvalid_enc = np_utils.to_categorical(yvalid)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:28:10.754370Z","iopub.execute_input":"2021-07-24T07:28:10.754759Z","iopub.status.idle":"2021-07-24T07:28:10.760955Z","shell.execute_reply.started":"2021-07-24T07:28:10.754728Z","shell.execute_reply":"2021-07-24T07:28:10.759351Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# create a simple 3 layer sequential neural net\nmodel = Sequential()\n\nmodel.add(Dense(300, input_dim=300, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(300, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:28:13.914114Z","iopub.execute_input":"2021-07-24T07:28:13.914519Z","iopub.status.idle":"2021-07-24T07:28:18.682352Z","shell.execute_reply.started":"2021-07-24T07:28:13.914471Z","shell.execute_reply":"2021-07-24T07:28:18.681289Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n          epochs=5, verbose=1, \n          validation_data=(xvalid_glove_scl, yvalid_enc))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:28:24.856466Z","iopub.execute_input":"2021-07-24T07:28:24.856886Z","iopub.status.idle":"2021-07-24T07:28:33.189104Z","shell.execute_reply.started":"2021-07-24T07:28:24.856841Z","shell.execute_reply":"2021-07-24T07:28:33.188064Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Epoch 1/5\n276/276 [==============================] - 4s 5ms/step - loss: 1.0918 - val_loss: 0.7932\nEpoch 2/5\n276/276 [==============================] - 1s 4ms/step - loss: 0.7493 - val_loss: 0.7454\nEpoch 3/5\n276/276 [==============================] - 1s 4ms/step - loss: 0.6823 - val_loss: 0.7305\nEpoch 4/5\n276/276 [==============================] - 1s 4ms/step - loss: 0.6510 - val_loss: 0.7240\nEpoch 5/5\n276/276 [==============================] - 1s 4ms/step - loss: 0.6109 - val_loss: 0.7349\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f95c027ed10>"},"metadata":{}}]},{"cell_type":"code","source":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 70\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:28:36.640172Z","iopub.execute_input":"2021-07-24T07:28:36.640615Z","iopub.status.idle":"2021-07-24T07:28:37.945268Z","shell.execute_reply.started":"2021-07-24T07:28:36.640576Z","shell.execute_reply":"2021-07-24T07:28:37.944227Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:28:43.179533Z","iopub.execute_input":"2021-07-24T07:28:43.179970Z","iopub.status.idle":"2021-07-24T07:28:43.273847Z","shell.execute_reply.started":"2021-07-24T07:28:43.179939Z","shell.execute_reply":"2021-07-24T07:28:43.272446Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"100%|██████████| 25943/25943 [00:00<00:00, 310286.52it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:28:47.606103Z","iopub.execute_input":"2021-07-24T07:28:47.606568Z","iopub.status.idle":"2021-07-24T07:28:47.958878Z","shell.execute_reply.started":"2021-07-24T07:28:47.606520Z","shell.execute_reply":"2021-07-24T07:28:47.957726Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:28:55.964859Z","iopub.execute_input":"2021-07-24T07:28:55.965246Z","iopub.status.idle":"2021-07-24T07:51:29.226187Z","shell.execute_reply.started":"2021-07-24T07:28:55.965213Z","shell.execute_reply":"2021-07-24T07:51:29.225085Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch 1/100\n35/35 [==============================] - 17s 381ms/step - loss: 1.0883 - val_loss: 0.9378\nEpoch 2/100\n35/35 [==============================] - 14s 399ms/step - loss: 0.9348 - val_loss: 0.8027\nEpoch 3/100\n35/35 [==============================] - 13s 372ms/step - loss: 0.8535 - val_loss: 0.7666\nEpoch 4/100\n35/35 [==============================] - 14s 414ms/step - loss: 0.8117 - val_loss: 0.7240\nEpoch 5/100\n35/35 [==============================] - 14s 391ms/step - loss: 0.7924 - val_loss: 0.6968\nEpoch 6/100\n35/35 [==============================] - 14s 391ms/step - loss: 0.7839 - val_loss: 0.7227\nEpoch 7/100\n35/35 [==============================] - 13s 376ms/step - loss: 0.7532 - val_loss: 0.6956\nEpoch 8/100\n35/35 [==============================] - 14s 389ms/step - loss: 0.7307 - val_loss: 0.6762\nEpoch 9/100\n35/35 [==============================] - 14s 393ms/step - loss: 0.7260 - val_loss: 0.6785\nEpoch 10/100\n35/35 [==============================] - 13s 381ms/step - loss: 0.7134 - val_loss: 0.6517\nEpoch 11/100\n35/35 [==============================] - 14s 399ms/step - loss: 0.7010 - val_loss: 0.6365\nEpoch 12/100\n35/35 [==============================] - 14s 393ms/step - loss: 0.6808 - val_loss: 0.6208\nEpoch 13/100\n35/35 [==============================] - 13s 374ms/step - loss: 0.6603 - val_loss: 0.6403\nEpoch 14/100\n35/35 [==============================] - 14s 396ms/step - loss: 0.6569 - val_loss: 0.6154\nEpoch 15/100\n35/35 [==============================] - 13s 373ms/step - loss: 0.6285 - val_loss: 0.6054\nEpoch 16/100\n35/35 [==============================] - 14s 408ms/step - loss: 0.6242 - val_loss: 0.5925\nEpoch 17/100\n35/35 [==============================] - 13s 375ms/step - loss: 0.6104 - val_loss: 0.6114\nEpoch 18/100\n35/35 [==============================] - 13s 378ms/step - loss: 0.5846 - val_loss: 0.6092\nEpoch 19/100\n35/35 [==============================] - 14s 395ms/step - loss: 0.5818 - val_loss: 0.5798\nEpoch 20/100\n35/35 [==============================] - 13s 377ms/step - loss: 0.5613 - val_loss: 0.5899\nEpoch 21/100\n35/35 [==============================] - 14s 406ms/step - loss: 0.5561 - val_loss: 0.5878\nEpoch 22/100\n35/35 [==============================] - 13s 383ms/step - loss: 0.5559 - val_loss: 0.5914\nEpoch 23/100\n35/35 [==============================] - 13s 369ms/step - loss: 0.5428 - val_loss: 0.5739\nEpoch 24/100\n35/35 [==============================] - 14s 392ms/step - loss: 0.5295 - val_loss: 0.5818\nEpoch 25/100\n35/35 [==============================] - 13s 382ms/step - loss: 0.5184 - val_loss: 0.5777\nEpoch 26/100\n35/35 [==============================] - 14s 388ms/step - loss: 0.5090 - val_loss: 0.5622\nEpoch 27/100\n35/35 [==============================] - 14s 390ms/step - loss: 0.4864 - val_loss: 0.5895\nEpoch 28/100\n35/35 [==============================] - 14s 397ms/step - loss: 0.4946 - val_loss: 0.5634\nEpoch 29/100\n35/35 [==============================] - 13s 370ms/step - loss: 0.4902 - val_loss: 0.5644\nEpoch 30/100\n35/35 [==============================] - 13s 382ms/step - loss: 0.4837 - val_loss: 0.6005\nEpoch 31/100\n35/35 [==============================] - 14s 400ms/step - loss: 0.4888 - val_loss: 0.5632\nEpoch 32/100\n35/35 [==============================] - 13s 371ms/step - loss: 0.4652 - val_loss: 0.5716\nEpoch 33/100\n35/35 [==============================] - 14s 392ms/step - loss: 0.4620 - val_loss: 0.6073\nEpoch 34/100\n35/35 [==============================] - 13s 387ms/step - loss: 0.4518 - val_loss: 0.5756\nEpoch 35/100\n35/35 [==============================] - 13s 370ms/step - loss: 0.4422 - val_loss: 0.5757\nEpoch 36/100\n35/35 [==============================] - 14s 409ms/step - loss: 0.4430 - val_loss: 0.5602\nEpoch 37/100\n35/35 [==============================] - 13s 379ms/step - loss: 0.4429 - val_loss: 0.5734\nEpoch 38/100\n35/35 [==============================] - 14s 402ms/step - loss: 0.4122 - val_loss: 0.5857\nEpoch 39/100\n35/35 [==============================] - 13s 373ms/step - loss: 0.4208 - val_loss: 0.5851\nEpoch 40/100\n35/35 [==============================] - 13s 372ms/step - loss: 0.4003 - val_loss: 0.5682\nEpoch 41/100\n35/35 [==============================] - 14s 392ms/step - loss: 0.4074 - val_loss: 0.5892\nEpoch 42/100\n35/35 [==============================] - 13s 378ms/step - loss: 0.3914 - val_loss: 0.5828\nEpoch 43/100\n35/35 [==============================] - 14s 409ms/step - loss: 0.3944 - val_loss: 0.5810\nEpoch 44/100\n35/35 [==============================] - 13s 370ms/step - loss: 0.3883 - val_loss: 0.5792\nEpoch 45/100\n35/35 [==============================] - 13s 377ms/step - loss: 0.3941 - val_loss: 0.5839\nEpoch 46/100\n35/35 [==============================] - 14s 391ms/step - loss: 0.3749 - val_loss: 0.5973\nEpoch 47/100\n35/35 [==============================] - 13s 371ms/step - loss: 0.3723 - val_loss: 0.5716\nEpoch 48/100\n35/35 [==============================] - 14s 400ms/step - loss: 0.3730 - val_loss: 0.5837\nEpoch 49/100\n35/35 [==============================] - 13s 378ms/step - loss: 0.3686 - val_loss: 0.6276\nEpoch 50/100\n35/35 [==============================] - 14s 403ms/step - loss: 0.3601 - val_loss: 0.6237\nEpoch 51/100\n35/35 [==============================] - 13s 381ms/step - loss: 0.3498 - val_loss: 0.6240\nEpoch 52/100\n35/35 [==============================] - 13s 377ms/step - loss: 0.3643 - val_loss: 0.5972\nEpoch 53/100\n35/35 [==============================] - 14s 407ms/step - loss: 0.3428 - val_loss: 0.5857\nEpoch 54/100\n35/35 [==============================] - 13s 377ms/step - loss: 0.3473 - val_loss: 0.6076\nEpoch 55/100\n35/35 [==============================] - 13s 383ms/step - loss: 0.3440 - val_loss: 0.6255\nEpoch 56/100\n35/35 [==============================] - 13s 384ms/step - loss: 0.3137 - val_loss: 0.6095\nEpoch 57/100\n35/35 [==============================] - 13s 378ms/step - loss: 0.3123 - val_loss: 0.6186\nEpoch 58/100\n35/35 [==============================] - 14s 392ms/step - loss: 0.3427 - val_loss: 0.6248\nEpoch 59/100\n35/35 [==============================] - 13s 384ms/step - loss: 0.3288 - val_loss: 0.6126\nEpoch 60/100\n35/35 [==============================] - 14s 403ms/step - loss: 0.3302 - val_loss: 0.6140\nEpoch 61/100\n35/35 [==============================] - 13s 368ms/step - loss: 0.3179 - val_loss: 0.6038\nEpoch 62/100\n35/35 [==============================] - 13s 369ms/step - loss: 0.3245 - val_loss: 0.6283\nEpoch 63/100\n35/35 [==============================] - 14s 390ms/step - loss: 0.3059 - val_loss: 0.6358\nEpoch 64/100\n35/35 [==============================] - 13s 374ms/step - loss: 0.3152 - val_loss: 0.6293\nEpoch 65/100\n35/35 [==============================] - 14s 412ms/step - loss: 0.3046 - val_loss: 0.6119\nEpoch 66/100\n35/35 [==============================] - 14s 387ms/step - loss: 0.2935 - val_loss: 0.6169\nEpoch 67/100\n35/35 [==============================] - 13s 375ms/step - loss: 0.2974 - val_loss: 0.6342\nEpoch 68/100\n35/35 [==============================] - 14s 390ms/step - loss: 0.3015 - val_loss: 0.6315\nEpoch 69/100\n35/35 [==============================] - 13s 364ms/step - loss: 0.2887 - val_loss: 0.6239\nEpoch 70/100\n35/35 [==============================] - 14s 407ms/step - loss: 0.2903 - val_loss: 0.6147\nEpoch 71/100\n35/35 [==============================] - 13s 378ms/step - loss: 0.2964 - val_loss: 0.6308\nEpoch 72/100\n35/35 [==============================] - 14s 399ms/step - loss: 0.2895 - val_loss: 0.6571\nEpoch 73/100\n35/35 [==============================] - 13s 376ms/step - loss: 0.2979 - val_loss: 0.6423\nEpoch 74/100\n35/35 [==============================] - 14s 388ms/step - loss: 0.2829 - val_loss: 0.6387\nEpoch 75/100\n35/35 [==============================] - 14s 390ms/step - loss: 0.2760 - val_loss: 0.6217\nEpoch 76/100\n35/35 [==============================] - 13s 378ms/step - loss: 0.2768 - val_loss: 0.6772\nEpoch 77/100\n35/35 [==============================] - 14s 390ms/step - loss: 0.2720 - val_loss: 0.6287\nEpoch 78/100\n35/35 [==============================] - 14s 391ms/step - loss: 0.2676 - val_loss: 0.6549\nEpoch 79/100\n35/35 [==============================] - 13s 380ms/step - loss: 0.2722 - val_loss: 0.6719\nEpoch 80/100\n35/35 [==============================] - 14s 395ms/step - loss: 0.2626 - val_loss: 0.6449\nEpoch 81/100\n35/35 [==============================] - 13s 378ms/step - loss: 0.2701 - val_loss: 0.6683\nEpoch 82/100\n35/35 [==============================] - 15s 418ms/step - loss: 0.2733 - val_loss: 0.6973\nEpoch 83/100\n35/35 [==============================] - 13s 372ms/step - loss: 0.2638 - val_loss: 0.6793\nEpoch 84/100\n35/35 [==============================] - 13s 376ms/step - loss: 0.2592 - val_loss: 0.6848\nEpoch 85/100\n35/35 [==============================] - 14s 389ms/step - loss: 0.2764 - val_loss: 0.6514\nEpoch 86/100\n35/35 [==============================] - 13s 376ms/step - loss: 0.2606 - val_loss: 0.6864\nEpoch 87/100\n35/35 [==============================] - 14s 412ms/step - loss: 0.2614 - val_loss: 0.7086\nEpoch 88/100\n35/35 [==============================] - 13s 387ms/step - loss: 0.2594 - val_loss: 0.6801\nEpoch 89/100\n35/35 [==============================] - 14s 392ms/step - loss: 0.2514 - val_loss: 0.7106\nEpoch 90/100\n35/35 [==============================] - 13s 365ms/step - loss: 0.2454 - val_loss: 0.7224\nEpoch 91/100\n35/35 [==============================] - 13s 374ms/step - loss: 0.2501 - val_loss: 0.6903\nEpoch 92/100\n35/35 [==============================] - 14s 394ms/step - loss: 0.2520 - val_loss: 0.7014\nEpoch 93/100\n35/35 [==============================] - 13s 376ms/step - loss: 0.2413 - val_loss: 0.6844\nEpoch 94/100\n35/35 [==============================] - 13s 384ms/step - loss: 0.2557 - val_loss: 0.6877\nEpoch 95/100\n35/35 [==============================] - 13s 372ms/step - loss: 0.2460 - val_loss: 0.7273\nEpoch 96/100\n35/35 [==============================] - 13s 381ms/step - loss: 0.2401 - val_loss: 0.7307\nEpoch 97/100\n35/35 [==============================] - 14s 402ms/step - loss: 0.2444 - val_loss: 0.7068\nEpoch 98/100\n35/35 [==============================] - 13s 374ms/step - loss: 0.2436 - val_loss: 0.7326\nEpoch 99/100\n35/35 [==============================] - 14s 392ms/step - loss: 0.2307 - val_loss: 0.7416\nEpoch 100/100\n35/35 [==============================] - 13s 368ms/step - loss: 0.2346 - val_loss: 0.7116\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f9640001810>"},"metadata":{}}]},{"cell_type":"code","source":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-24T07:55:21.269375Z","iopub.execute_input":"2021-07-24T07:55:21.269844Z","iopub.status.idle":"2021-07-24T08:00:48.279818Z","shell.execute_reply.started":"2021-07-24T07:55:21.269810Z","shell.execute_reply":"2021-07-24T08:00:48.278478Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Epoch 1/100\n35/35 [==============================] - 17s 407ms/step - loss: 1.0905 - val_loss: 0.8683\nEpoch 2/100\n35/35 [==============================] - 13s 377ms/step - loss: 0.8930 - val_loss: 0.7925\nEpoch 3/100\n35/35 [==============================] - 14s 401ms/step - loss: 0.8469 - val_loss: 0.7372\nEpoch 4/100\n35/35 [==============================] - 14s 389ms/step - loss: 0.8059 - val_loss: 0.7494\nEpoch 5/100\n35/35 [==============================] - 13s 372ms/step - loss: 0.8053 - val_loss: 0.7453\nEpoch 6/100\n35/35 [==============================] - 14s 393ms/step - loss: 0.7771 - val_loss: 0.7051\nEpoch 7/100\n35/35 [==============================] - 13s 377ms/step - loss: 0.7412 - val_loss: 0.6882\nEpoch 8/100\n35/35 [==============================] - 14s 381ms/step - loss: 0.7328 - val_loss: 0.6764\nEpoch 9/100\n35/35 [==============================] - 14s 399ms/step - loss: 0.7288 - val_loss: 0.6777\nEpoch 10/100\n35/35 [==============================] - 13s 378ms/step - loss: 0.7016 - val_loss: 0.6436\nEpoch 11/100\n35/35 [==============================] - 14s 394ms/step - loss: 0.6775 - val_loss: 0.6298\nEpoch 12/100\n35/35 [==============================] - 14s 394ms/step - loss: 0.6471 - val_loss: 0.6108\nEpoch 13/100\n35/35 [==============================] - 14s 390ms/step - loss: 0.6114 - val_loss: 0.6038\nEpoch 14/100\n35/35 [==============================] - 13s 371ms/step - loss: 0.6185 - val_loss: 0.6181\nEpoch 15/100\n35/35 [==============================] - 13s 374ms/step - loss: 0.5942 - val_loss: 0.6406\nEpoch 16/100\n35/35 [==============================] - 14s 403ms/step - loss: 0.5700 - val_loss: 0.6037\nEpoch 17/100\n35/35 [==============================] - 13s 383ms/step - loss: 0.5481 - val_loss: 0.5985\nEpoch 18/100\n35/35 [==============================] - 14s 394ms/step - loss: 0.5523 - val_loss: 0.5796\nEpoch 19/100\n35/35 [==============================] - 13s 369ms/step - loss: 0.5389 - val_loss: 0.5960\nEpoch 20/100\n35/35 [==============================] - 13s 379ms/step - loss: 0.5032 - val_loss: 0.5800\nEpoch 21/100\n35/35 [==============================] - 14s 403ms/step - loss: 0.5098 - val_loss: 0.5755\nEpoch 22/100\n35/35 [==============================] - 13s 365ms/step - loss: 0.4852 - val_loss: 0.5916\nEpoch 23/100\n35/35 [==============================] - 14s 390ms/step - loss: 0.4596 - val_loss: 0.6041\nEpoch 24/100\n35/35 [==============================] - 13s 375ms/step - loss: 0.4456 - val_loss: 0.6455\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f963a4d04d0>"},"metadata":{}}]},{"cell_type":"code","source":"# A simple bidirectional LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-24T08:02:08.752450Z","iopub.execute_input":"2021-07-24T08:02:08.752868Z","iopub.status.idle":"2021-07-24T08:10:30.616385Z","shell.execute_reply.started":"2021-07-24T08:02:08.752836Z","shell.execute_reply":"2021-07-24T08:10:30.615409Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Epoch 1/100\n35/35 [==============================] - 32s 746ms/step - loss: 1.0815 - val_loss: 0.8678\nEpoch 2/100\n35/35 [==============================] - 26s 752ms/step - loss: 0.8962 - val_loss: 0.7904\nEpoch 3/100\n35/35 [==============================] - 27s 765ms/step - loss: 0.8277 - val_loss: 0.7443\nEpoch 4/100\n35/35 [==============================] - 27s 766ms/step - loss: 0.8015 - val_loss: 0.7274\nEpoch 5/100\n35/35 [==============================] - 26s 748ms/step - loss: 0.7780 - val_loss: 0.7232\nEpoch 6/100\n35/35 [==============================] - 25s 722ms/step - loss: 0.7698 - val_loss: 0.7089\nEpoch 7/100\n35/35 [==============================] - 27s 752ms/step - loss: 0.7528 - val_loss: 0.6925\nEpoch 8/100\n35/35 [==============================] - 26s 746ms/step - loss: 0.7331 - val_loss: 0.6694\nEpoch 9/100\n35/35 [==============================] - 27s 762ms/step - loss: 0.7000 - val_loss: 0.6543\nEpoch 10/100\n35/35 [==============================] - 26s 740ms/step - loss: 0.7000 - val_loss: 0.6367\nEpoch 11/100\n35/35 [==============================] - 26s 738ms/step - loss: 0.6553 - val_loss: 0.6264\nEpoch 12/100\n35/35 [==============================] - 26s 739ms/step - loss: 0.6486 - val_loss: 0.6208\nEpoch 13/100\n35/35 [==============================] - 26s 737ms/step - loss: 0.6162 - val_loss: 0.6241\nEpoch 14/100\n35/35 [==============================] - 26s 735ms/step - loss: 0.5979 - val_loss: 0.6127\nEpoch 15/100\n35/35 [==============================] - 26s 743ms/step - loss: 0.5962 - val_loss: 0.5977\nEpoch 16/100\n35/35 [==============================] - 25s 725ms/step - loss: 0.5803 - val_loss: 0.5927\nEpoch 17/100\n35/35 [==============================] - 26s 741ms/step - loss: 0.5525 - val_loss: 0.6653\nEpoch 18/100\n35/35 [==============================] - 26s 736ms/step - loss: 0.5474 - val_loss: 0.6198\nEpoch 19/100\n35/35 [==============================] - 26s 746ms/step - loss: 0.5151 - val_loss: 0.5998\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f9638c371d0>"},"metadata":{}}]},{"cell_type":"code","source":"# GRU with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:10:43.131431Z","iopub.execute_input":"2021-07-24T08:10:43.131851Z","iopub.status.idle":"2021-07-24T08:21:42.597963Z","shell.execute_reply.started":"2021-07-24T08:10:43.131821Z","shell.execute_reply":"2021-07-24T08:21:42.596864Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Epoch 1/100\n35/35 [==============================] - 30s 727ms/step - loss: 1.0997 - val_loss: 0.9387\nEpoch 2/100\n35/35 [==============================] - 24s 697ms/step - loss: 0.9529 - val_loss: 0.8190\nEpoch 3/100\n35/35 [==============================] - 25s 710ms/step - loss: 0.8750 - val_loss: 0.7428\nEpoch 4/100\n35/35 [==============================] - 24s 682ms/step - loss: 0.8317 - val_loss: 0.7576\nEpoch 5/100\n35/35 [==============================] - 24s 687ms/step - loss: 0.8072 - val_loss: 0.7262\nEpoch 6/100\n35/35 [==============================] - 24s 683ms/step - loss: 0.7737 - val_loss: 0.7094\nEpoch 7/100\n35/35 [==============================] - 24s 676ms/step - loss: 0.7537 - val_loss: 0.6850\nEpoch 8/100\n35/35 [==============================] - 25s 715ms/step - loss: 0.7327 - val_loss: 0.6623\nEpoch 9/100\n35/35 [==============================] - 24s 694ms/step - loss: 0.7263 - val_loss: 0.6903\nEpoch 10/100\n35/35 [==============================] - 24s 686ms/step - loss: 0.7003 - val_loss: 0.6483\nEpoch 11/100\n35/35 [==============================] - 24s 686ms/step - loss: 0.6795 - val_loss: 0.6269\nEpoch 12/100\n35/35 [==============================] - 24s 688ms/step - loss: 0.6592 - val_loss: 0.6224\nEpoch 13/100\n35/35 [==============================] - 24s 693ms/step - loss: 0.6258 - val_loss: 0.6090\nEpoch 14/100\n35/35 [==============================] - 25s 708ms/step - loss: 0.6253 - val_loss: 0.6009\nEpoch 15/100\n35/35 [==============================] - 24s 700ms/step - loss: 0.5920 - val_loss: 0.6100\nEpoch 16/100\n35/35 [==============================] - 24s 688ms/step - loss: 0.5998 - val_loss: 0.6022\nEpoch 17/100\n35/35 [==============================] - 24s 682ms/step - loss: 0.5791 - val_loss: 0.5904\nEpoch 18/100\n35/35 [==============================] - 25s 716ms/step - loss: 0.5677 - val_loss: 0.5721\nEpoch 19/100\n35/35 [==============================] - 23s 659ms/step - loss: 0.5481 - val_loss: 0.5681\nEpoch 20/100\n35/35 [==============================] - 24s 699ms/step - loss: 0.5374 - val_loss: 0.5724\nEpoch 21/100\n35/35 [==============================] - 25s 702ms/step - loss: 0.5171 - val_loss: 0.5630\nEpoch 22/100\n35/35 [==============================] - 24s 694ms/step - loss: 0.5074 - val_loss: 0.5635\nEpoch 23/100\n35/35 [==============================] - 24s 676ms/step - loss: 0.4934 - val_loss: 0.5585\nEpoch 24/100\n35/35 [==============================] - 25s 702ms/step - loss: 0.4746 - val_loss: 0.5448\nEpoch 25/100\n35/35 [==============================] - 24s 693ms/step - loss: 0.4705 - val_loss: 0.5681\nEpoch 26/100\n35/35 [==============================] - 24s 699ms/step - loss: 0.4556 - val_loss: 0.5450\nEpoch 27/100\n35/35 [==============================] - 24s 668ms/step - loss: 0.4399 - val_loss: 0.5556\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f9637f17bd0>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Ensembling","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport pandas as pd\nimport os\nimport sys\nimport logging\n\nlogging.basicConfig(\nlevel=logging.DEBUG,\nformat='[%(asctime)s] %(levelname)s %(message)s',\ndatefmt='%H:%M:%S', stream=sys.stdout)\nlogger = logging.getLogger(__name__)\n\n\nclass Ensembler(object):\n    def __init__(self,model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n                lower_is_better=False, save_path=None):\n        \"\"\"\n        Ensembler init function\n        :param model_dict: model dictionary, see README for its format\n        :param num_folds: the number of folds for ensembling\n        :param task_type: classification or regression\n        :param optimize: the function to optimize for, e.g. AUC, logloss, etc. Must have two arguments y_test and y_pred\n        :param lower_is_better: is lower value of optimization function better or higher\n        :param save_path: path to which model pickles will be dumped to along with generated predictions, or None\n        \"\"\"\n        \n        self.model_dict=model_dict\n        self.levels = len(self.model_dict)\n        self.num_folds = num_folds\n        self.task_type = task_type\n        self.optimize = optimize\n        self.lower_is_better = lower_is_better\n        self.save_path = save_path\n\n        self.training_data = None\n        self.test_data = None\n        self.y = None\n        self.lbl_enc = None\n        self.y_enc = None\n        self.train_prediction_dict = None\n        self.test_prediction_dict = None\n        self.num_classes = None\n\n    def fit(self, training_data, y, lentrain):\n        \"\"\"\n        :param training_data: training data in tabular format\n        :param y: binary, multi-class or regression\n        :return: chain of models to be used in prediction\n        \"\"\"\n\n        self.training_data = training_data\n        self.y = y\n\n        if self.task_type == 'classification':\n            self.num_classes = len(np.unique(self.y))\n            logger.info(\"Found %d classes\", self.num_classes)\n            self.lbl_enc = LabelEncoder()\n            self.y_enc = self.lbl_enc.fit_transform(self.y)\n            kf = StratifiedKFold(n_splits=self.num_folds)\n            train_prediction_shape = (lentrain, self.num_classes)\n        else:\n            self.num_classes = -1\n            self.y_enc = self.y\n            kf = KFold(n_splits=self.num_folds)\n            train_prediction_shape = (lentrain, 1)\n\n        self.train_prediction_dict = {}\n        for level in range(self.levels):\n            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n\n        for level in range(self.levels):\n\n            if level == 0:\n                temp_train = self.training_data\n            else:\n                temp_train = self.train_prediction_dict[level - 1]\n\n            for model_num, model in enumerate(self.model_dict[level]):\n                validation_scores = []\n                foldnum = 1\n                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n                    logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n\n                    if level != 0:\n                        l_training_data = temp_train[train_index]\n                        l_validation_data = temp_train[valid_index]\n                        model.fit(l_training_data, self.y_enc[train_index])\n                    else:\n                        l0_training_data = temp_train[0][model_num]\n                        if type(l0_training_data) == list:\n                            l_training_data = [x[train_index] for x in l0_training_data]\n                            l_validation_data = [x[valid_index] for x in l0_training_data]\n                        else:\n                            l_training_data = l0_training_data[train_index]\n                            l_validation_data = l0_training_data[valid_index]\n                        model.fit(l_training_data, self.y_enc[train_index])\n\n                    logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n\n                    if self.task_type == 'classification':\n                        temp_train_predictions = model.predict_proba(l_validation_data)\n                        self.train_prediction_dict[level][valid_index,\n                        (model_num * self.num_classes):(model_num * self.num_classes) +\n                                                       self.num_classes] = temp_train_predictions\n\n                    else:\n                        temp_train_predictions = model.predict(l_validation_data)\n                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n                    validation_scores.append(validation_score)\n                    logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n                                validation_score)\n                    foldnum += 1\n                avg_score = np.mean(validation_scores)\n                std_score = np.std(validation_scores)\n                logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n                            avg_score, std_score)\n\n            logger.info(\"Saving predictions for level # %d\", level)\n            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n                                        index=False, header=None)\n\n        return self.train_prediction_dict\n\n    def predict(self, test_data, lentest):\n        self.test_data = test_data\n        if self.task_type == 'classification':\n            test_prediction_shape = (lentest, self.num_classes)\n        else:\n            test_prediction_shape = (lentest, 1)\n\n        self.test_prediction_dict = {}\n        for level in range(self.levels):\n            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n        self.test_data = test_data\n        for level in range(self.levels):\n            if level == 0:\n                temp_train = self.training_data\n                temp_test = self.test_data\n            else:\n                temp_train = self.train_prediction_dict[level - 1]\n                temp_test = self.test_prediction_dict[level - 1]\n\n            for model_num, model in enumerate(self.model_dict[level]):\n\n                logger.info(\"Training Fulldata Level %d. Model # %d\", level, model_num)\n                if level == 0:\n                    model.fit(temp_train[0][model_num], self.y_enc)\n                else:\n                    model.fit(temp_train, self.y_enc)\n\n                logger.info(\"Predicting Test Level %d. Model # %d\", level, model_num)\n\n                if self.task_type == 'classification':\n                    if level == 0:\n                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n                    else:\n                        temp_test_predictions = model.predict_proba(temp_test)\n                    self.test_prediction_dict[level][:, (model_num * self.num_classes): (model_num * self.num_classes) +\n                                                                                        self.num_classes] = temp_test_predictions\n\n                else:\n                    if level == 0:\n                        temp_test_predictions = model.predict(temp_test[0][model_num])\n                    else:\n                        temp_test_predictions = model.predict(temp_test)\n                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n\n            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n                                       index=False, header=None)\n\n        return self.test_prediction_dict","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:22:13.368808Z","iopub.execute_input":"2021-07-24T08:22:13.369222Z","iopub.status.idle":"2021-07-24T08:22:13.405115Z","shell.execute_reply.started":"2021-07-24T08:22:13.369192Z","shell.execute_reply":"2021-07-24T08:22:13.404043Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# specify the data to be used for every level of ensembling:\ntrain_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv], 1: [xtrain_glove]}\ntest_data_dict = {0: [xvalid_tfv, xvalid_ctv, xvalid_tfv, xvalid_ctv], 1: [xvalid_glove]}\n\nmodel_dict = {0: [LogisticRegression(solver='liblinear'), LogisticRegression(solver='liblinear'), MultinomialNB(alpha=0.1), MultinomialNB()],\n\n              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n\nens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n                optimize=multiclass_logloss, lower_is_better=True, save_path='')\n\nens.fit(train_data_dict, ytrain, lentrain=xtrain_glove.shape[0])\npreds = ens.predict(test_data_dict, lentest=xvalid_glove.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:26:05.726349Z","iopub.execute_input":"2021-07-24T08:26:05.726767Z","iopub.status.idle":"2021-07-24T08:27:13.217342Z","shell.execute_reply.started":"2021-07-24T08:26:05.726734Z","shell.execute_reply":"2021-07-24T08:27:13.216171Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[08:26:09] WARNING: ../src/learner.cc:573: \nParameters: { \"silent\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[08:26:09] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[08:26:24] WARNING: ../src/learner.cc:573: \nParameters: { \"silent\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[08:26:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[08:26:37] WARNING: ../src/learner.cc:573: \nParameters: { \"silent\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[08:26:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[08:26:53] WARNING: ../src/learner.cc:573: \nParameters: { \"silent\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[08:26:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","output_type":"stream"}]},{"cell_type":"code","source":"# check error:\nmulticlass_logloss(yvalid, preds[1])","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:27:26.947529Z","iopub.execute_input":"2021-07-24T08:27:26.947914Z","iopub.status.idle":"2021-07-24T08:27:26.957290Z","shell.execute_reply.started":"2021-07-24T08:27:26.947881Z","shell.execute_reply":"2021-07-24T08:27:26.955874Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"0.0030739385695921118"},"metadata":{}}]},{"cell_type":"markdown","source":"https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle/notebook","metadata":{}}]}